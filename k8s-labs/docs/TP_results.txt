TP2

root@dakube:~# uname -a
Linux dakube 6.1.0-41-arm64 #1 SMP Debian 6.1.158-1 (2025-11-09) aarch64 GNU/Linux
root@dakube:~# lsb_release -a||cat /etc/os-release
-bash: lsb_release: command not found
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"
root@dakube:~# free -h
               total        used        free      shared  buff/cache   available
Mem:           1.8Gi       753Mi       566Mi       1.3Mi       662Mi       1.1Gi
Swap:          2.0Gi          0B       2.0Gi
root@dakube:~# df -h /
Filesystem      Size  Used Avail Use% Mounted on
/dev/mmcblk0p2   59G  7.1G   49G  13% /
root@dakube:~# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host noprefixroute
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether dc:a6:32:29:12:36 brd ff:ff:ff:ff:ff:ff
    altname end0
    inet 192.168.1.5/24 brd 192.168.1.255 scope global dynamic eth0
       valid_lft 39341sec preferred_lft 39341sec
    inet6 2a01:e0a:f7c:44e0:dea6:32ff:fe29:1236/64 scope global dynamic mngtmpaddr
       valid_lft 86074sec preferred_lft 86074sec
    inet6 fe80::dea6:32ff:fe29:1236/64 scope link
       valid_lft forever preferred_lft forever
3: wlan0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000
    link/ether dc:a6:32:29:12:37 brd ff:ff:ff:ff:ff:ff
4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:a5:0a:b5:7e brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
5: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default
    link/ether 2e:08:f1:66:11:81 brd ff:ff:ff:ff:ff:ff
    inet 10.42.1.0/32 scope global flannel.1
       valid_lft forever preferred_lft forever
    inet6 fe80::2c08:f1ff:fe66:1181/64 scope link
       valid_lft forever preferred_lft forever
6: cni0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default qlen 1000
    link/ether 0a:03:a8:99:d6:ff brd ff:ff:ff:ff:ff:ff
    inet 10.42.1.1/24 brd 10.42.1.255 scope global cni0
       valid_lft forever preferred_lft forever
    inet6 fe80::803:a8ff:fe99:d6ff/64 scope link
       valid_lft forever preferred_lft forever
7: vetha1f495a9@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP group default qlen 1000
    link/ether ea:24:ea:9a:6a:9a brd ff:ff:ff:ff:ff:ff link-netns cni-8d39ce74-83c6-d2e3-b531-fcdc4eb5085d
    inet6 fe80::e824:eaff:fe9a:6a9a/64 scope link
       valid_lft forever preferred_lft forever
8: vethb6bd0997@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP group default qlen 1000
    link/ether 6a:c6:75:aa:48:31 brd ff:ff:ff:ff:ff:ff link-netns cni-b6256a50-49d7-efdc-80d4-d91daab54ae5
    inet6 fe80::68c6:75ff:feaa:4831/64 scope link
       valid_lft forever preferred_lft forever
9: vethd1543197@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP group default qlen 1000
    link/ether 3e:bb:b1:f8:a0:fe brd ff:ff:ff:ff:ff:ff link-netns cni-df0090a1-9d1a-c09e-5f92-d4a8b22eea3c
    inet6 fe80::3cbb:b1ff:fef8:a0fe/64 scope link
       valid_lft forever preferred_lft forever

root@dakube:~# apt update
Hit:1 http://download.proxmox.com/debian/pve bookworm InRelease
Hit:2 http://deb.debian.org/debian bookworm InRelease
Get:3 http://security.debian.org/debian-security bookworm-security InRelease [48.0 kB]
Get:4 http://deb.debian.org/debian bookworm-updates InRelease [55.4 kB]
Get:5 http://security.debian.org/debian-security bookworm-security/main arm64 Packages [286 kB]
Fetched 389 kB in 2s (219 kB/s)
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
All packages are up to date.
root@dakube:~# apt full-upgrade -y
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
Calculating upgrade... Done
The following packages were automatically installed and are no longer required:
  libglib2.0-0 libglib2.0-data libicu72 libxml2 shared-mime-info wmdocker xdg-user-dirs
Use 'apt autoremove' to remove them.
The following packages will be REMOVED:
  linux-image-6.1.0-13-arm64 linux-image-6.1.0-32-arm64
0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.
After this operation, 712 MB disk space will be freed.
(Reading database ... 44074 files and directories currently installed.)
Removing linux-image-6.1.0-13-arm64 (6.1.55-1) ...
/etc/kernel/postrm.d/initramfs-tools:
update-initramfs: Deleting /boot/initrd.img-6.1.0-13-arm64
/etc/kernel/postrm.d/z50-raspi-firmware:
raspi-firmware: deleting obsolete /boot/firmware/vmlinuz-6.1.0-13-arm64 (no longer in /boot)
raspi-firmware: deleting obsolete /boot/firmware/initrd.img-6.1.0-13-arm64 (no longer in /boot)
Removing linux-image-6.1.0-32-arm64 (6.1.129-1) ...
/etc/kernel/postrm.d/initramfs-tools:
update-initramfs: Deleting /boot/initrd.img-6.1.0-32-arm64
/etc/kernel/postrm.d/z50-raspi-firmware:
raspi-firmware: deleting obsolete /boot/firmware/vmlinuz-6.1.0-32-arm64 (no longer in /boot)
raspi-firmware: deleting obsolete /boot/firmware/initrd.img-6.1.0-32-arm64 (no longer in /boot)

# swapon --show
NAME      TYPE SIZE USED PRIO
/swapfile file   2G   0B   -2

# cat /etc/fstab
# The root file system has fs_passno=1 as per fstab(5) for automatic fsck.
LABEL=RASPIROOT / ext4 rw 0 1
# All other file systems have fs_passno=2 as per fstab(5) for automatic fsck.
LABEL=RASPIFIRM /boot/firmware vfat rw 0 2
#/swapfile none swap sw 0 0
dabox:/volume1/KubeData /mnt/nfs/k3s nfs defaults 0 0

# cat /boot/cmdline.txt
cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1

# grep cgroup /boot/cmdline.txt
cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1
root@dakube:~# grep cgroup /proc/cmdline
root@dakube:~#

root@dakube:~# curl -fsSL https://get.docker.com | sh
# Executing docker install script, commit: f381ee68b32e515bb4dc034b339266aff1fbc460
Warning: the "docker" command appears to already exist on this system.

If you already have Docker installed, this script can cause trouble, which is
why we're displaying this warning and provide the opportunity to cancel the
installation.

If you installed the current Docker package using this script and are using it
again to update Docker, you can ignore this message, but be aware that the
script resets any custom changes in the deb and rpm repo configuration
files to match the parameters passed to the script.

You may press Ctrl+C now to abort this script.
+ sleep 20
+ sh -c apt-get -qq update >/dev/null
+ sh -c DEBIAN_FRONTEND=noninteractive apt-get -y -qq install ca-certificates curl >/dev/null
+ sh -c install -m 0755 -d /etc/apt/keyrings
+ sh -c curl -fsSL "https://download.docker.com/linux/debian/gpg" -o /etc/apt/keyrings/docker.asc
+ sh -c chmod a+r /etc/apt/keyrings/docker.asc
+ sh -c echo "deb [arch=arm64 signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/debian bookworm stable" > /etc/apt/sources.list.d/docker.list
+ sh -c apt-get -qq update >/dev/null
+ sh -c DEBIAN_FRONTEND=noninteractive apt-get -y -qq install docker-ce docker-ce-cli containerd.io docker-compose-plugin docker-ce-rootless-extras docker-buildx-plugin docker-model-plugin >/dev/null
Scanning processes...
Scanning processor microcode...
Scanning linux images...
Using systemd to manage Docker service
+ sh -c systemctl enable --now docker.service
  UNIT                                           LOAD   ACTIVE SUB       DESCRIPTION
  proc-sys-fs-binfmt_misc.automount              loaded active running   Arbitrary Executable File Formats File System …
  sys-devices-platform-emmc2bus-fe340000.mmc-mm… loaded active plugged   /sys/devices/platform/emmc2bus/fe340000.mmc/mm…
  sys-devices-platform-emmc2bus-fe340000.mmc-mm… loaded active plugged   /sys/devices/platform/emmc2bus/fe340000.mmc/mm…
  sys-devices-platform-emmc2bus-fe340000.mmc-mm… loaded active plugged   /sys/devices/platform/emmc2bus/fe340000.mmc/mm…
  sys-devices-platform-scb-fd580000.ethernet-ne… loaded active plugged   /sys/devices/platform/scb/fd580000.ethernet/ne…
  sys-devices-platform-scb-fd580000.ethernet-un… loaded active plugged   /sys/devices/platform/scb/fd580000.ethernet/un…
  sys-devices-platform-soc-fe00b840.mailbox-bcm… loaded active plugged   /sys/devices/platform/soc/fe00b840.mailbox/bcm…
  sys-devices-platform-soc-fe201000.serial-seri… loaded active plugged   /sys/devices/platform/soc/fe201000.serial/seri…
  sys-devices-platform-soc-fe215040.serial-tty-… loaded active plugged   /sys/devices/platform/soc/fe215040.serial/tty/…
  sys-devices-platform-soc-fe300000.mmc-mmc_hos… loaded active plugged   /sys/devices/platform/soc/fe300000.mmc/mmc_hos…
  sys-devices-platform-soc-fe980000.usb-udc-fe9… loaded active plugged   /sys/devices/platform/soc/fe980000.usb/udc/fe9…
  sys-devices-platform-soc-fef00700.hdmi-sound-… loaded active plugged   /sys/devices/platform/soc/fef00700.hdmi/sound/…
  sys-devices-platform-soc-fef05700.hdmi-sound-… loaded active plugged   /sys/devices/platform/soc/fef05700.hdmi/sound/…
  sys-devices-virtual-misc-rfkill.device         loaded active plugged   /sys/devices/virtual/misc/rfkill
  sys-devices-virtual-net-cni0.device            loaded active plugged   /sys/devices/virtual/net/cni0
  sys-devices-virtual-net-docker0.device         loaded active plugged   /sys/devices/virtual/net/docker0
  sys-devices-virtual-net-flannel.1.device       loaded active plugged   /sys/devices/virtual/net/flannel.1
  sys-devices-virtual-net-veth754519be.device    loaded active plugged   /sys/devices/virtual/net/veth754519be
  sys-devices-virtual-net-vethcefaeded.device    loaded active plugged   /sys/devices/virtual/net/vethcefaeded
  sys-devices-virtual-net-vethdab01af8.device    loaded active plugged   /sys/devices/virtual/net/vethdab01af8
  sys-module-configfs.device                     loaded active plugged   /sys/module/configfs
  sys-module-fuse.device                         loaded active plugged   /sys/module/fuse
  sys-subsystem-bluetooth-devices-hci0.device    loaded active plugged   /sys/subsystem/bluetooth/devices/hci0
  sys-subsystem-net-devices-cni0.device          loaded active plugged   /sys/subsystem/net/devices/cni0
  sys-subsystem-net-devices-docker0.device       loaded active plugged   /sys/subsystem/net/devices/docker0
  sys-subsystem-net-devices-eth0.device          loaded active plugged   /sys/subsystem/net/devices/eth0
  sys-subsystem-net-devices-flannel.1.device     loaded active plugged   /sys/subsystem/net/devices/flannel.1
  sys-subsystem-net-devices-veth754519be.device  loaded active plugged   /sys/subsystem/net/devices/veth754519be
  sys-subsystem-net-devices-vethcefaeded.device  loaded active plugged   /sys/subsystem/net/devices/vethcefaeded
  sys-subsystem-net-devices-vethdab01af8.device  loaded active plugged   /sys/subsystem/net/devices/vethdab01af8
  sys-subsystem-net-devices-wlan0.device         loaded active plugged   /sys/subsystem/net/devices/wlan0
  -.mount                                        loaded active mounted   Root Mount
  boot-firmware.mount                            loaded active mounted   /boot/firmware
  dev-hugepages.mount                            loaded active mounted   Huge Pages File System
  dev-mqueue.mount                               loaded active mounted   POSIX Message Queue File System
  mnt-nfs-k3s.mount                              loaded active mounted   /mnt/nfs/k3s
  proc-sys-fs-binfmt_misc.mount                  loaded active mounted   Arbitrary Executable File Formats File System
  run-credentials-systemd\x2dsysctl.service.mou… loaded active mounted   /run/credentials/systemd-sysctl.service
  run-credentials-systemd\x2dsysusers.service.m… loaded active mounted   /run/credentials/systemd-sysusers.service
  run-credentials-systemd\x2dtmpfiles\x2dsetup.… loaded active mounted   /run/credentials/systemd-tmpfiles-setup.service
  run-credentials-systemd\x2dtmpfiles\x2dsetup\… loaded active mounted   /run/credentials/systemd-tmpfiles-setup-dev.se…
  run-k3s-containerd-io.containerd.grpc.v1.cri-… loaded active mounted   /run/k3s/containerd/io.containerd.grpc.v1.cri/…
  run-k3s-containerd-io.containerd.grpc.v1.cri-… loaded active mounted   /run/k3s/containerd/io.containerd.grpc.v1.cri/…
  run-k3s-containerd-io.containerd.grpc.v1.cri-… loaded active mounted   /run/k3s/containerd/io.containerd.grpc.v1.cri/…
  run-k3s-containerd-io.containerd.runtime.v2.t… loaded active mounted   /run/k3s/containerd/io.containerd.runtime.v2.t…
  run-k3s-containerd-io.containerd.runtime.v2.t… loaded active mounted   /run/k3s/containerd/io.containerd.runtime.v2.t…
  run-k3s-containerd-io.containerd.runtime.v2.t… loaded active mounted   /run/k3s/containerd/io.containerd.runtime.v2.t…
  run-k3s-containerd-io.containerd.runtime.v2.t… loaded active mounted   /run/k3s/containerd/io.containerd.runtime.v2.t…
  run-k3s-containerd-io.containerd.runtime.v2.t… loaded active mounted   /run/k3s/containerd/io.containerd.runtime.v2.t…
  run-k3s-containerd-io.containerd.runtime.v2.t… loaded active mounted   /run/k3s/containerd/io.containerd.runtime.v2.t…
  run-netns-cni\x2d23d10938\x2d359e\x2df55e\x2d… loaded active mounted   /run/netns/cni-23d10938-359e-f55e-3d0a-9c53be5…
  run-netns-cni\x2d6a191c47\x2d1d2c\x2d95a3\x2d… loaded active mounted   /run/netns/cni-6a191c47-1d2c-95a3-b566-7b446dd…
  run-netns-cni\x2d7753d4cb\x2d5b8d\x2d531b\x2d… loaded active mounted   /run/netns/cni-7753d4cb-5b8d-531b-8ff2-2c54c0c…
  run-rpc_pipefs.mount                           loaded active mounted   RPC Pipe File System
  run-user-1000.mount                            loaded active mounted   /run/user/1000
  sys-fs-fuse-connections.mount                  loaded active mounted   FUSE Control File System
  sys-kernel-config.mount                        loaded active mounted   Kernel Configuration File System
  sys-kernel-debug.mount                         loaded active mounted   Kernel Debug File System
  sys-kernel-tracing.mount                       loaded active mounted   Kernel Trace File System
  var-lib-kubelet-pods-2c5d91bf\x2d4a9e\x2d46f3… loaded active mounted   /var/lib/kubelet/pods/2c5d91bf-4a9e-46f3-9cca-…
  var-lib-kubelet-pods-7a21603a\x2da8bd\x2d4827… loaded active mounted   /var/lib/kubelet/pods/7a21603a-a8bd-4827-b9de-…
  var-lib-kubelet-pods-b1108a4a\x2deb98\x2d4ca7… loaded active mounted   /var/lib/kubelet/pods/b1108a4a-eb98-4ca7-be9d-…
  systemd-ask-password-console.path              loaded active waiting   Dispatch Password Requests to Console Director…
  systemd-ask-password-wall.path                 loaded active waiting   Forward Password Requests to Wall Directory Wa…
  cri-containerd-015838bf7505a8fc1ec5873f5adbd5… loaded active running   libcontainer container 015838bf7505a8fc1ec5873…
  cri-containerd-04be514b59948773f8928875936355… loaded active running   libcontainer container 04be514b59948773f892887…
  cri-containerd-b290cc0adf7a1eb0d5d75827ebab7f… loaded active running   libcontainer container b290cc0adf7a1eb0d5d7582…
  cri-containerd-ca85e3604a62432729232cea44ab9a… loaded active running   libcontainer container ca85e3604a62432729232ce…
  cri-containerd-caeb80c03a1dd9cd1dc9ecbc64a01f… loaded active running   libcontainer container caeb80c03a1dd9cd1dc9ecb…
  cri-containerd-fc12a95b7bbf4ca94fdf3ad99ccb17… loaded active running   libcontainer container fc12a95b7bbf4ca94fdf3ad…
  init.scope                                     loaded active running   System and Service Manager
  session-1.scope                                loaded active running   Session 1 of User rouls
  apparmor.service                               loaded active exited    Load AppArmor profiles
  console-setup.service                          loaded active exited    Set console font and keymap
  containerd.service                             loaded active running   containerd container runtime
  cron.service                                   loaded active running   Regular background program processing daemon
  dbus.service                                   loaded active running   D-Bus System Message Bus
  docker.service                                 loaded active running   Docker Application Container Engine
  getty@tty1.service                             loaded active running   Getty on tty1
  ifupdown-pre.service                           loaded active exited    Helper to synchronize boot up for ifupdown
  k3s.service                                    loaded active running   Lightweight Kubernetes
  keyboard-setup.service                         loaded active exited    Set the console keyboard layout
  kmod-static-nodes.service                      loaded active exited    Create List of Static Device Nodes
  networking.service                             loaded active exited    Raise network interfaces
  rpc-statd-notify.service                       loaded active exited    Notify NFS peers of a restart
  rpcbind.service                                loaded active running   RPC bind portmap service
  serial-getty@ttyS1.service                     loaded active running   Serial Getty on ttyS1
  ssh.service                                    loaded active running   OpenBSD Secure Shell server
  systemd-binfmt.service                         loaded active exited    Set Up Additional Binary Formats
  systemd-fsck@dev-disk-by\x2dlabel-RASPIFIRM.s… loaded active exited    File System Check on /dev/disk/by-label/RASPIF…
  systemd-journal-flush.service                  loaded active exited    Flush Journal to Persistent Storage
  systemd-journald.service                       loaded active running   Journal Service
  systemd-logind.service                         loaded active running   User Login Management
  systemd-modules-load.service                   loaded active exited    Load Kernel Modules
  systemd-random-seed.service                    loaded active exited    Load/Save Random Seed
  systemd-remount-fs.service                     loaded active exited    Remount Root and Kernel File Systems
  systemd-sysctl.service                         loaded active exited    Apply Kernel Variables
  systemd-sysusers.service                       loaded active exited    Create System Users
  systemd-timesyncd.service                      loaded active running   Network Time Synchronization
  systemd-tmpfiles-setup-dev.service             loaded active exited    Create Static Device Nodes in /dev
  systemd-tmpfiles-setup.service                 loaded active exited    Create System Files and Directories
  systemd-udev-trigger.service                   loaded active exited    Coldplug All udev Devices
  systemd-udevd.service                          loaded active running   Rule-based Manager for Device Events and Files
  systemd-update-utmp.service                    loaded active exited    Record System Boot/Shutdown in UTMP
  systemd-user-sessions.service                  loaded active exited    Permit User Sessions
  user-runtime-dir@1000.service                  loaded active exited    User Runtime Directory /run/user/1000
  user@1000.service                              loaded active running   User Manager for UID 1000
  wpa_supplicant.service                         loaded active running   WPA supplicant
  -.slice                                        loaded active active    Root Slice
  kubepods-besteffort-pod7a21603a_a8bd_4827_b9d… loaded active active    libcontainer container kubepods-besteffort-pod…
  kubepods-besteffort.slice                      loaded active active    libcontainer container kubepods-besteffort.sli…
  kubepods-burstable-pod2c5d91bf_4a9e_46f3_9cca… loaded active active    libcontainer container kubepods-burstable-pod2…
  kubepods-burstable-podb1108a4a_eb98_4ca7_be9d… loaded active active    libcontainer container kubepods-burstable-podb…
  kubepods-burstable.slice                       loaded active active    libcontainer container kubepods-burstable.slice
  kubepods.slice                                 loaded active active    libcontainer container kubepods.slice
  system-getty.slice                             loaded active active    Slice /system/getty
  system-modprobe.slice                          loaded active active    Slice /system/modprobe
  system-serial\x2dgetty.slice                   loaded active active    Slice /system/serial-getty
  system-systemd\x2dfsck.slice                   loaded active active    Slice /system/systemd-fsck
  system.slice                                   loaded active active    System Slice
  user-1000.slice                                loaded active active    User Slice of UID 1000
  user.slice                                     loaded active active    User and Session Slice
  dbus.socket                                    loaded active running   D-Bus System Message Bus Socket
  docker.socket                                  loaded active running   Docker Socket for the API
  rpcbind.socket                                 loaded active running   RPCbind Server Activation Socket
  systemd-fsckd.socket                           loaded active listening fsck to fsckd communication Socket
  systemd-initctl.socket                         loaded active listening initctl Compatibility Named Pipe
  systemd-journald-audit.socket                  loaded active running   Journal Audit Socket
  systemd-journald-dev-log.socket                loaded active running   Journal Socket (/dev/log)
  systemd-journald.socket                        loaded active running   Journal Socket
  systemd-rfkill.socket                          loaded active listening Load/Save RF Kill Switch Status /dev/rfkill Wa…
  systemd-udevd-control.socket                   loaded active running   udev Control Socket
  systemd-udevd-kernel.socket                    loaded active running   udev Kernel Socket
  basic.target                                   loaded active active    Basic System
  bluetooth.target                               loaded active active    Bluetooth Support
  cryptsetup.target                              loaded active active    Local Encrypted Volumes
  getty.target                                   loaded active active    Login Prompts
  graphical.target                               loaded active active    Graphical Interface
  integritysetup.target                          loaded active active    Local Integrity Protected Volumes
  local-fs-pre.target                            loaded active active    Preparation for Local File Systems
  local-fs.target                                loaded active active    Local File Systems
  multi-user.target                              loaded active active    Multi-User System
  network-online.target                          loaded active active    Network is Online
  network.target                                 loaded active active    Network
  nfs-client.target                              loaded active active    NFS client services
  paths.target                                   loaded active active    Path Units
  remote-fs-pre.target                           loaded active active    Preparation for Remote File Systems
  remote-fs.target                               loaded active active    Remote File Systems
  rpc_pipefs.target                              loaded active active    rpc_pipefs.target
  rpcbind.target                                 loaded active active    RPC Port Mapper
  slices.target                                  loaded active active    Slice Units
  sockets.target                                 loaded active active    Socket Units
  sound.target                                   loaded active active    Sound Card
  swap.target                                    loaded active active    Swaps
  sysinit.target                                 loaded active active    System Initialization
  time-set.target                                loaded active active    System Time Set
  timers.target                                  loaded active active    Timer Units
  usb-gadget.target                              loaded active active    Hardware activated USB gadget
  veritysetup.target                             loaded active active    Local Verity Protected Volumes
  apt-daily-upgrade.timer                        loaded active waiting   Daily apt upgrade and clean activities
  apt-daily.timer                                loaded active waiting   Daily apt download activities
  dpkg-db-backup.timer                           loaded active waiting   Daily dpkg database backup timer
  e2scrub_all.timer                              loaded active waiting   Periodic ext4 Online Metadata Check for All Fi…
  fstrim.timer                                   loaded active waiting   Discard unused blocks once a week
  logrotate.timer                                loaded active waiting   Daily rotation of log files
  systemd-tmpfiles-clean.timer                   loaded active waiting   Daily Cleanup of Temporary Directories

LOAD   = Reflects whether the unit definition was properly loaded.
ACTIVE = The high-level unit activation state, i.e. generalization of SUB.
SUB    = The low-level unit activation state, values depend on unit type.
166 loaded units listed. Pass --all to see loaded but inactive units, too.
To show all installed unit files use 'systemctl list-unit-files'.
INFO: Docker daemon enabled and started

+ sh -c docker version
Client: Docker Engine - Community
 Version:           29.1.4
 API version:       1.52
 Go version:        go1.25.5
 Git commit:        0e6fee6
 Built:             Thu Jan  8 19:57:28 2026
 OS/Arch:           linux/arm64
 Context:           default

Server: Docker Engine - Community
 Engine:
  Version:          29.1.4
  API version:      1.52 (minimum version 1.44)
  Go version:       go1.25.5
  Git commit:       08440b6
  Built:            Thu Jan  8 19:57:28 2026
  OS/Arch:          linux/arm64
  Experimental:     false
 containerd:
  Version:          v2.2.1
  GitCommit:        dea7da592f5d1d2b7755e3a161be07f43fad8f75
 runc:
  Version:          1.3.4
  GitCommit:        v1.3.4-0-gd6d73eb8
 docker-init:
  Version:          0.19.0
  GitCommit:        de40ad0

================================================================================

To run Docker as a non-privileged user, consider setting up the
Docker daemon in rootless mode for your user:

    dockerd-rootless-setuptool.sh install

Visit https://docs.docker.com/go/rootless/ to learn about rootless mode.


To run the Docker daemon as a fully privileged service, but granting non-root
users access, refer to https://docs.docker.com/go/daemon-access/

WARNING: Access to the remote API on a privileged Docker daemon is equivalent
         to root access on the host. Refer to the 'Docker daemon attack surface'
         documentation for details: https://docs.docker.com/go/attack-surface/

================================================================================
$ docker version
Client: Docker Engine - Community
 Version:           29.1.4
 API version:       1.52
 Go version:        go1.25.5
 Git commit:        0e6fee6
 Built:             Thu Jan  8 19:57:28 2026
 OS/Arch:           linux/arm64
 Context:           default

Server: Docker Engine - Community
 Engine:
  Version:          29.1.4
  API version:      1.52 (minimum version 1.44)
  Go version:       go1.25.5
  Git commit:       08440b6
  Built:            Thu Jan  8 19:57:28 2026
  OS/Arch:          linux/arm64
  Experimental:     false
 containerd:
  Version:          v2.2.1
  GitCommit:        dea7da592f5d1d2b7755e3a161be07f43fad8f75
 runc:
  Version:          1.3.4
  GitCommit:        v1.3.4-0-gd6d73eb8
 docker-init:
  Version:          0.19.0
  GitCommit:        de40ad0
$ docker run --rm hello-world

Hello from Docker!
This message shows that your installation appears to be working correctly.

To generate this message, Docker took the following steps:
 1. The Docker client contacted the Docker daemon.
 2. The Docker daemon pulled the "hello-world" image from the Docker Hub.
    (arm64v8)
 3. The Docker daemon created a new container from that image which runs the
    executable that produces the output you are currently reading.
 4. The Docker daemon streamed that output to the Docker client, which sent it
    to your terminal.

To try something more ambitious, you can run an Ubuntu container with:
 $ docker run -it ubuntu bash

Share images, automate workflows, and more with a free Docker ID:
 https://hub.docker.com/

For more examples and ideas, visit:
 https://docs.docker.com/get-started/


# ufw status
Status: active

To                         Action      From
--                         ------      ----
22/tcp                     ALLOW       Anywhere                  
22/tcp (v6)                ALLOW       Anywhere (v6)         


# docker info | grep -E 'Architecture|Cgroup'
swapon --show
 Cgroup Driver: systemd
 Cgroup Version: 2
 Architecture: aarch64

TP2.5

# ss -tulpn
Netid        State         Recv-Q        Send-Q               Local Address:Port                Peer Address:Port       Process
udp          UNCONN        0             0                          0.0.0.0:68                       0.0.0.0:*           users:(("dhclient",pid=530,fd=7))
udp          UNCONN        0             0                          0.0.0.0:111                      0.0.0.0:*           users:(("rpcbind",pid=434,fd=5),("systemd",pid=1,fd=93))
udp          UNCONN        0             0                          0.0.0.0:8472                     0.0.0.0:*
udp          UNCONN        0             0                             [::]:111                         [::]:*           users:(("rpcbind",pid=434,fd=7),("systemd",pid=1,fd=96))
tcp          LISTEN        0             4096                     127.0.0.1:10249                    0.0.0.0:*           users:(("k3s-server",pid=601,fd=217))
tcp          LISTEN        0             4096                     127.0.0.1:10248                    0.0.0.0:*           users:(("k3s-server",pid=601,fd=236))
tcp          LISTEN        0             4096                     127.0.0.1:10259                    0.0.0.0:*           users:(("k3s-server",pid=601,fd=226))
tcp          LISTEN        0             4096                     127.0.0.1:10258                    0.0.0.0:*           users:(("k3s-server",pid=601,fd=181))
tcp          LISTEN        0             4096                     127.0.0.1:10257                    0.0.0.0:*           users:(("k3s-server",pid=601,fd=268))
tcp          LISTEN        0             4096                     127.0.0.1:10256                    0.0.0.0:*           users:(("k3s-server",pid=601,fd=213))
tcp          LISTEN        0             4096                     127.0.0.1:6444                     0.0.0.0:*           users:(("k3s-server",pid=601,fd=17))
tcp          LISTEN        0             64                         0.0.0.0:40995                    0.0.0.0:*
tcp          LISTEN        0             128                        0.0.0.0:22                       0.0.0.0:*           users:(("sshd",pid=3783,fd=3))
tcp          LISTEN        0             4096                       0.0.0.0:111                      0.0.0.0:*           users:(("rpcbind",pid=434,fd=4),("systemd",pid=1,fd=92))
tcp          LISTEN        0             4096                     127.0.0.1:10010                    0.0.0.0:*           users:(("containerd",pid=817,fd=6))
tcp          LISTEN        0             64                            [::]:37093                       [::]:*
tcp          LISTEN        0             4096                             *:10255                          *:*           users:(("k3s-server",pid=601,fd=230))
tcp          LISTEN        0             4096                             *:10250                          *:*           users:(("k3s-server",pid=601,fd=233))
tcp          LISTEN        0             128                           [::]:22                          [::]:*           users:(("sshd",pid=3783,fd=4))
tcp          LISTEN        0             4096                          [::]:111                         [::]:*           users:(("rpcbind",pid=434,fd=6),("systemd",pid=1,fd=94))
tcp          LISTEN        0             4096                             *:6443                           *:*           users:(("k3s-server",pid=601,fd=13))
root@dakube:~# aa-status
apparmor module is loaded.
9 profiles are loaded.
9 profiles are in enforce mode.
   /usr/lib/NetworkManager/nm-dhcp-client.action
   /usr/lib/NetworkManager/nm-dhcp-helper
   /usr/lib/connman/scripts/dhclient-script
   /{,usr/}sbin/dhclient
   cri-containerd.apparmor.d
   docker-default
   lsb_release
   nvidia_modprobe
   nvidia_modprobe//kmod
0 profiles are in complain mode.
0 profiles are in kill mode.
0 profiles are in unconfined mode.
4 processes have profiles defined.
4 processes are in enforce mode.
   /usr/sbin/dhclient (530) /{,usr/}sbin/dhclient
   /usr/bin/local-path-provisioner (1857) cri-containerd.apparmor.d
   /coredns (1865) cri-containerd.apparmor.d
   /metrics-server (1898) cri-containerd.apparmor.d
0 processes are in complain mode.
0 processes are unconfined but have a profile defined.
0 processes are in mixed mode.
0 processes are in kill mode.

# ps -ef | grep dockerd
root        3137       1  0 10:43 ?        00:00:01 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
root        5675    3638  0 11:28 pts/0    00:00:00 grep dockerd

# systemctl list-unit-files --state=enabled
UNIT FILE                          STATE   PRESET
apparmor.service                   enabled enabled
console-setup.service              enabled enabled
containerd.service                 enabled enabled
cron.service                       enabled enabled
docker.service                     enabled enabled
e2scrub_reap.service               enabled enabled
getty@.service                     enabled enabled
k3s.service                        enabled enabled
keyboard-setup.service             enabled enabled
networking.service                 enabled enabled
rpcbind.service                    enabled enabled
rpi-generate-ssh-host-keys.service enabled enabled
rpi-set-sysconf.service            enabled enabled
ssh.service                        enabled enabled
systemd-pstore.service             enabled enabled
systemd-timesyncd.service          enabled enabled
ufw.service                        enabled enabled
wpa_supplicant.service             enabled enabled
docker.socket                      enabled enabled
rpcbind.socket                     enabled enabled
nfs-client.target                  enabled enabled
remote-fs.target                   enabled enabled
apt-daily-upgrade.timer            enabled enabled
apt-daily.timer                    enabled enabled
dpkg-db-backup.timer               enabled enabled
e2scrub_all.timer                  enabled enabled
fstrim.timer                       enabled enabled
logrotate.timer                    enabled enabled

# sysctl --system
* Applying /usr/lib/sysctl.d/50-pid-max.conf ...
* Applying /etc/sysctl.d/99-k8s-hardening.conf ...
* Applying /usr/lib/sysctl.d/99-protect-links.conf ...
* Applying /etc/sysctl.d/99-sysctl.conf ...
* Applying /etc/sysctl.conf ...
kernel.pid_max = 4194304
net.ipv4.conf.all.accept_redirects = 0
net.ipv6.conf.all.accept_redirects = 0
net.ipv4.conf.all.rp_filter = 1
fs.suid_dumpable = 0
fs.protected_symlinks = 1
fs.protected_hardlinks = 1
fs.protected_fifos = 1
fs.protected_hardlinks = 1
fs.protected_regular = 2
fs.protected_symlinks = 1


 kubectl get nodes -o wide
NAME        STATUS     ROLES                  AGE    VERSION        INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION   CONTAINER-RUNTIME
dakube      Ready      control-plane,master   276d   v1.32.3+k3s1   192.168.1.5   <none>        Debian GNU/Linux 12 (bookworm)   6.1.0-41-arm64   containerd://2.0.4-k3s2
daproxmox   NotReady   control-plane,master   276d   v1.32.3+k3s1   192.168.1.5   <none>        Debian GNU/Linux 12 (bookworm)   6.1.0-32-arm64   containerd://2.0.4-k3s2
root@dakube:~# kubectl describe node dakube
Name:               dakube
Roles:              control-plane,master
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/instance-type=k3s
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=dakube
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/control-plane=true
                    node-role.kubernetes.io/master=true
                    node.kubernetes.io/instance-type=k3s
Annotations:        alpha.kubernetes.io/provided-node-ip: 192.168.1.5,2a01:e0a:f7c:44e0:dea6:32ff:fe29:1236
                    flannel.alpha.coreos.com/backend-data: {"VNI":1,"VtepMAC":"2e:08:f1:66:11:81"}
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: true
                    flannel.alpha.coreos.com/public-ip: 192.168.1.5
                    k3s.io/hostname: dakube
                    k3s.io/internal-ip: 192.168.1.5,2a01:e0a:f7c:44e0:dea6:32ff:fe29:1236
                    k3s.io/node-args: ["server","--disable","traefik","--write-kubeconfig-mode","644"]
                    k3s.io/node-config-hash: Y735WCEK5OA7WRV7XHI7EBVBQNG2C7ABWJO2R25H2YIAEAJQVALA====
                    k3s.io/node-env: {}
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 08 Apr 2025 22:57:09 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  dakube
  AcquireTime:     <unset>
  RenewTime:       Sat, 10 Jan 2026 11:31:57 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 10 Jan 2026 11:31:14 +0000   Tue, 08 Apr 2025 22:57:09 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 10 Jan 2026 11:31:14 +0000   Tue, 08 Apr 2025 22:57:09 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 10 Jan 2026 11:31:14 +0000   Tue, 08 Apr 2025 22:57:09 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 10 Jan 2026 11:31:14 +0000   Tue, 08 Apr 2025 22:57:09 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.1.5
  InternalIP:  2a01:e0a:f7c:44e0:dea6:32ff:fe29:1236
  Hostname:    dakube
Capacity:
  cpu:                4
  ephemeral-storage:  60855576Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             1886144Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  59200304287
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             1886144Ki
  pods:               110
System Info:
  Machine ID:                 009b349ff625405a85550a4b44493392
  System UUID:                009b349ff625405a85550a4b44493392
  Boot ID:                    c1d33c67-8f31-4579-8dd8-9136aa07a047
  Kernel Version:             6.1.0-41-arm64
  OS Image:                   Debian GNU/Linux 12 (bookworm)
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  containerd://2.0.4-k3s2
  Kubelet Version:            v1.32.3+k3s1
  Kube-Proxy Version:         v1.32.3+k3s1
PodCIDR:                      10.42.1.0/24
PodCIDRs:                     10.42.1.0/24
ProviderID:                   k3s://dakube
Non-terminated Pods:          (3 in total)
  Namespace                   Name                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                       ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-ff8999cc5-2hdqq                    100m (2%)     0 (0%)      70Mi (3%)        170Mi (9%)     275d
  kube-system                 local-path-provisioner-774c6665dc-7mh4z    0 (0%)        0 (0%)      0 (0%)           0 (0%)         275d
  kube-system                 metrics-server-6f4c6675d5-2kqg6            100m (2%)     0 (0%)      70Mi (3%)        0 (0%)         275d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                200m (5%)   0 (0%)
  memory             140Mi (7%)  170Mi (9%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
  hugepages-32Mi     0 (0%)      0 (0%)
  hugepages-64Ki     0 (0%)      0 (0%)
Events:
  Type     Reason                             Age   From             Message
  ----     ------                             ----  ----             -------
  Normal   Starting                           52m   kube-proxy
  Normal   Starting                           57m   kube-proxy
  Warning  PossibleMemoryBackedVolumesOnDisk  57m   kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           57m   kubelet          Starting kubelet.
  Normal   NodeAllocatableEnforced            57m   kubelet          Updated Node Allocatable limit across pods
  Warning  Rebooted                           57m   kubelet          Node dakube has been rebooted, boot id: f8c378c5-8405-40ca-87cb-b0aad57cca0e
  Normal   NodeHasSufficientMemory            57m   kubelet          Node dakube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              57m   kubelet          Node dakube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               57m   kubelet          Node dakube status is now: NodeHasSufficientPID
  Normal   NodePasswordValidationComplete     57m   k3s-supervisor   Deferred node password secret validation complete
  Normal   RegisteredNode                     56m   node-controller  Node dakube event: Registered Node dakube in Controller
  Normal   Starting                           52m   kubelet          Starting kubelet.
  Warning  InvalidDiskCapacity                52m   kubelet          invalid capacity 0 on image filesystem
  Normal   NodeAllocatableEnforced            52m   kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            52m   kubelet          Node dakube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              52m   kubelet          Node dakube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               52m   kubelet          Node dakube status is now: NodeHasSufficientPID
  Warning  Rebooted                           52m   kubelet          Node dakube has been rebooted, boot id: c1d33c67-8f31-4579-8dd8-9136aa07a047
  Normal   NodePasswordValidationComplete     51m   k3s-supervisor   Deferred node password secret validation complete
  Normal   RegisteredNode                     51m   node-controller  Node dakube event: Registered Node dakube in Controller





TP 3

# docker run -d --name test alpine sleep 300
Unable to find image 'alpine:latest' locally
latest: Pulling from library/alpine
f6b4fb944634: Pull complete
Digest: sha256:865b95f46d98cf867a156fe4a135ad3fe50d2056aa3f25ed31662dff6da4eb62
Status: Downloaded newer image for alpine:latest
3c93c30ffd2aed33036df2c213ebc38b2f7e14a7aa478bed3182bec7505fe4d2
root@dakube:~# docker ps
CONTAINER ID   IMAGE     COMMAND       CREATED         STATUS         PORTS     NAMES
3c93c30ffd2a   alpine    "sleep 300"   7 seconds ago   Up 6 seconds             test
root@dakube:~# docker top ps
Error response from daemon: No such container: ps
root@dakube:~# docker top test
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                25551               25524               0                   21:14               ?                   00:00:00            sleep 300
root@dakube:~# ps -ef |grep sleep
root       25551   25524  0 21:14 ?        00:00:00 sleep 300
root       25615   25409  0 21:14 pts/0    00:00:00 grep sleep
root@dakube:~# docker exec test ps -ef
PID   USER     TIME  COMMAND
    1 root      0:00 sleep 300
    7 root      0:00 ps -ef
root@dakube:~# ps -ef
UID          PID    PPID  C STIME TTY          TIME CMD
root           1       0  0 10:39 ?        00:00:15 /sbin/init
root           2       0  0 10:39 ?        00:00:00 [kthreadd]
root           3       2  0 10:39 ?        00:00:00 [rcu_gp]
root           4       2  0 10:39 ?        00:00:00 [rcu_par_gp]
root           5       2  0 10:39 ?        00:00:00 [slub_flushwq]
root           6       2  0 10:39 ?        00:00:00 [netns]
root          10       2  0 10:39 ?        00:00:00 [mm_percpu_wq]
root          11       2  0 10:39 ?        00:00:00 [rcu_tasks_rude_kthread]
root          12       2  0 10:39 ?        00:00:00 [rcu_tasks_trace_kthread]
root          13       2  0 10:39 ?        00:00:01 [ksoftirqd/0]
root          14       2  0 10:39 ?        00:00:21 [rcu_sched]
root          15       2  0 10:39 ?        00:00:00 [migration/0]
root          17       2  0 10:39 ?        00:00:00 [cpuhp/0]
root          18       2  0 10:39 ?        00:00:00 [cpuhp/1]
root          19       2  0 10:39 ?        00:00:00 [migration/1]
root          20       2  0 10:39 ?        00:00:00 [ksoftirqd/1]
root          23       2  0 10:39 ?        00:00:00 [cpuhp/2]
root          24       2  0 10:39 ?        00:00:00 [migration/2]
root          25       2  0 10:39 ?        00:00:00 [ksoftirqd/2]
root          28       2  0 10:39 ?        00:00:00 [cpuhp/3]
root          29       2  0 10:39 ?        00:00:00 [migration/3]
root          30       2  0 10:39 ?        00:00:01 [ksoftirqd/3]
root          34       2  0 10:39 ?        00:00:00 [kdevtmpfs]
root          35       2  0 10:39 ?        00:00:00 [inet_frag_wq]
root          36       2  0 10:39 ?        00:00:00 [kauditd]
root          37       2  0 10:39 ?        00:00:00 [khungtaskd]
root          38       2  0 10:39 ?        00:00:00 [oom_reaper]
root          40       2  0 10:39 ?        00:00:00 [writeback]
root          41       2  0 10:39 ?        00:00:02 [kcompactd0]
root          42       2  0 10:39 ?        00:00:00 [ksmd]
root          44       2  0 10:39 ?        00:00:04 [khugepaged]
root          45       2  0 10:39 ?        00:00:00 [kintegrityd]
root          46       2  0 10:39 ?        00:00:00 [kblockd]
root          47       2  0 10:39 ?        00:00:00 [blkcg_punt_bio]
root          48       2  0 10:39 ?        00:00:00 [tpm_dev_wq]
root          49       2  0 10:39 ?        00:00:00 [edac-poller]
root          50       2  0 10:39 ?        00:00:00 [devfreq_wq]
root          51       2  0 10:39 ?        00:00:00 [watchdogd]
root          53       2  0 10:39 ?        00:00:01 [kworker/1:1H-kblockd]
root          56       2  0 10:39 ?        00:00:00 [kswapd0]
root          63       2  0 10:39 ?        00:00:00 [kthrotld]
root          66       2  0 10:39 ?        00:00:00 [irq/23-aerdrv]
root          68       2  0 10:39 ?        00:00:00 [mld]
root          69       2  0 10:39 ?        00:00:00 [ipv6_addrconf]
root          74       2  0 10:39 ?        00:00:00 [kstrp]
root          79       2  0 10:39 ?        00:00:00 [zswap-shrink]
root          80       2  0 10:39 ?        00:00:00 [kworker/u9:0-hci0]
root         135       2  0 10:39 ?        00:00:01 [kworker/2:1H-kblockd]
root         144       2  0 10:39 ?        00:00:00 [sdhci]
root         145       2  0 10:39 ?        00:00:00 [sdhci]
root         146       2  0 10:39 ?        00:00:00 [irq/35-mmc0]
root         147       2  0 10:39 ?        00:00:00 [irq/35-mmc1]
root         152       2  0 10:39 ?        00:00:00 [mmc_complete]
root         153       2  0 10:39 ?        00:00:00 [ptp0]
root         155       2  0 10:39 ?        00:00:03 [kworker/0:2H-kblockd]
root         177       2  0 10:39 ?        00:00:01 [jbd2/mmcblk1p2-8]
root         178       2  0 10:39 ?        00:00:00 [ext4-rsv-conver]
root         233       1  0 10:39 ?        00:00:02 /lib/systemd/systemd-journald
root         255       1  0 10:39 ?        00:00:00 /lib/systemd/systemd-udevd
root         290       2  0 10:39 ?        00:00:00 [v3d_bin]
root         291       2  0 10:39 ?        00:00:00 [v3d_render]
root         292       2  0 10:39 ?        00:00:00 [v3d_tfu]
root         293       2  0 10:39 ?        00:00:00 [v3d_csd]
root         294       2  0 10:39 ?        00:00:00 [v3d_cache_clean]
root         295       2  0 10:39 ?        00:00:00 [hwrng]
root         296       2  0 10:39 ?        00:00:00 [vchiq-slot/0]
root         297       2  0 10:39 ?        00:00:00 [vchiq-recy/0]
root         298       2  0 10:39 ?        00:00:00 [vchiq-sync/0]
root         319       2  0 10:39 ?        00:00:00 [cfg80211]
root         344       2  0 10:39 ?        00:00:00 [brcmf_wq/mmc0:0]
root         364       2  0 10:39 ?        00:00:00 [brcmf_wdog/mmc0:0001:1]
root         386       2  0 10:39 ?        00:00:05 [sugov:0]
root         389       2  0 10:39 ?        00:00:00 [vchiq-keep/0]
_rpc         434       1  0 10:39 ?        00:00:00 /sbin/rpcbind -f -w
root         436       2  0 10:39 ?        00:00:00 [rpciod]
root         438       2  0 10:39 ?        00:00:00 [xprtiod]
root         440       2  0 10:39 ?        00:00:00 [irq/43-vc4 hdmi hpd connected]
root         445       2  0 10:39 ?        00:00:00 [irq/44-vc4 hdmi hpd disconnected]
root         447       2  0 10:39 ?        00:00:00 [cec-vc4-hdmi-0]
root         448       2  0 10:39 ?        00:00:00 [irq/45-vc4 hdmi cec rx]
root         449       2  0 10:39 ?        00:00:00 [irq/46-vc4 hdmi cec tx]
root         455       2  0 10:39 ?        00:00:00 [irq/47-vc4 hdmi hpd connected]
root         456       2  0 10:39 ?        00:00:00 [irq/48-vc4 hdmi hpd disconnected]
root         457       2  0 10:39 ?        00:00:00 [cec-vc4-hdmi-1]
root         458       2  0 10:39 ?        00:00:00 [irq/49-vc4 hdmi cec rx]
root         459       2  0 10:39 ?        00:00:00 [irq/50-vc4 hdmi cec tx]
systemd+     460       1  0 10:39 ?        00:00:00 /lib/systemd/systemd-timesyncd
root         461       2  0 10:39 ?        00:00:00 [card1-crtc0]
root         462       2  0 10:39 ?        00:00:00 [card1-crtc1]
root         463       2  0 10:39 ?        00:00:00 [card1-crtc2]
root         464       2  0 10:39 ?        00:00:00 [card1-crtc3]
root         465       2  0 10:39 ?        00:00:00 [card1-crtc4]
root         494       2  0 10:39 ?        00:00:00 [kworker/u9:2-hci0]
message+     508       1  0 10:39 ?        00:00:03 /usr/bin/dbus-daemon --system --address=systemd: --nofork --nopidfile --systemd-activation --syslog-only
root         519       1  0 10:39 ?        00:00:00 /lib/systemd/systemd-logind
root         530       1  0 10:39 ?        00:00:00 dhclient -4 -v -i -pf /run/dhclient.eth0.pid -lf /var/lib/dhcp/dhclient.eth0.leases -I -df /var/lib/dhcp/dhclient6.eth0.leases eth0
root         598       2  0 10:39 ?        00:00:00 [nfsiod]
root         601       1 11 10:39 ?        01:16:17 /usr/local/bin/k3s server
root         617       2  0 10:39 ?        00:00:00 [NFSv4 callback]
root         624       1  0 10:39 ?        00:00:00 /usr/sbin/cron -f
root         626       1  0 10:39 tty1     00:00:00 /sbin/agetty -o -p -- \u --noclear - linux
root         627       1  0 10:39 ttyS1    00:00:00 /sbin/agetty -o -p -- \u --keep-baud 115200,57600,38400,9600 - vt220
root         817     601  1 10:40 ?        00:08:18 containerd
root        1655       1  0 10:40 ?        00:00:56 /var/lib/rancher/k3s/data/d94a66173941bf5aa5e030399bb4920efcca85113a35ce46908399199866769f/bin/containerd-shim-runc-v2 -namespace k8s.io -id caeb80c03a1dd9cd
root        1683       1  0 10:40 ?        00:00:56 /var/lib/rancher/k3s/data/d94a66173941bf5aa5e030399bb4920efcca85113a35ce46908399199866769f/bin/containerd-shim-runc-v2 -namespace k8s.io -id 015838bf7505a8fc
65535       1730    1655  0 10:40 ?        00:00:00 /pause
65535       1737    1683  0 10:40 ?        00:00:00 /pause
root        1748       1  0 10:40 ?        00:00:54 /var/lib/rancher/k3s/data/d94a66173941bf5aa5e030399bb4920efcca85113a35ce46908399199866769f/bin/containerd-shim-runc-v2 -namespace k8s.io -id b290cc0adf7a1eb0
65535       1794    1748  0 10:40 ?        00:00:00 /pause
root        1857    1655  0 10:40 ?        00:00:11 local-path-provisioner start --config /etc/config/config.json
65532       1865    1683  0 10:40 ?        00:03:56 /coredns -conf /etc/coredns/Corefile
rouls       1898    1748  2 10:40 ?        00:15:45 /metrics-server --cert-dir=/tmp --secure-port=10250 --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --kubelet-use-node-status-port --metric-
root        3092       1  0 10:43 ?        00:00:58 /usr/bin/containerd
root        3137       1  0 10:43 ?        00:00:15 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
root        3783       1  0 10:48 ?        00:00:00 sshd: /usr/sbin/sshd -D [listener] 0 of 10-100 startups
root        4173       2  0 10:50 ?        00:00:00 [kworker/3:2H-kblockd]
root       14295       2  0 15:40 ?        00:00:03 [kworker/1:0-mm_percpu_wq]
root       17313       2  0 17:10 ?        00:00:00 [kworker/0:0-events]
root       20665       2  0 18:50 ?        00:00:00 [kworker/2:0-events]
root       24215       2  0 20:37 ?        00:00:00 [kworker/u8:0-flush-179:0]
root       24322       2  0 20:40 ?        00:00:00 [kworker/3:0-events_power_efficient]
root       24682       2  0 20:50 ?        00:00:00 [kworker/1:2-cgroup_free]
root       25017       2  0 21:00 ?        00:00:00 [kworker/0:0H]
root       25105       2  0 21:03 ?        00:00:00 [kworker/u8:1-events_unbound]
root       25169       2  0 21:05 ?        00:00:00 [kworker/3:2]
root       25193       2  0 21:05 ?        00:00:00 [kworker/0:2-events]
root       25220       2  0 21:06 ?        00:00:00 [kworker/2:2-events]
root       25248       2  0 21:08 ?        00:00:00 [kworker/3:1H]
root       25270       2  0 21:08 ?        00:00:00 [kworker/2:0H-kblockd]
root       25302       2  0 21:09 ?        00:00:00 [kworker/u8:2-flush-179:0]
root       25362       2  0 21:10 ?        00:00:00 [kworker/1:2H]
root       25371    3783  0 21:11 ?        00:00:00 sshd: rouls [priv]
rouls      25374       1  0 21:11 ?        00:00:00 /lib/systemd/systemd --user
rouls      25375   25374  0 21:11 ?        00:00:00 (sd-pam)
root       25376       2  0 21:11 ?        00:00:00 [kworker/1:1-events]
rouls      25386   25371  0 21:11 ?        00:00:00 sshd: rouls@pts/0
rouls      25387   25386  0 21:11 pts/0    00:00:00 -sh
root       25392   25387  0 21:11 pts/0    00:00:00 su -
root       25409   25392  0 21:11 pts/0    00:00:00 -bash
root       25412       2  0 21:11 ?        00:00:00 [kworker/u8:3-events_unbound]
root       25425       2  0 21:12 ?        00:00:00 [kworker/2:1-events]
root       25440       2  0 21:12 ?        00:00:00 [kworker/0:1-mm_percpu_wq]
root       25477       2  0 21:13 ?        00:00:00 [kworker/3:0H-kblockd]
root       25524       1  0 21:14 ?        00:00:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 3c93c30ffd2aed33036df2c213ebc38b2f7e14a7aa478bed3182bec7505fe4d2 -address /run/containerd/containerd.soc
root       25551   25524  0 21:14 ?        00:00:00 sleep 300
root       25613       2  0 21:14 ?        00:00:00 [kworker/0:1H-kblockd]
root       25695   25409 99 21:15 pts/0    00:00:00 ps -ef
root@dakube:~# docker ps
CONTAINER ID   IMAGE     COMMAND       CREATED         STATUS         PORTS     NAMES
3c93c30ffd2a   alpine    "sleep 300"   2 minutes ago   Up 2 minutes             test
root@dakube:~#
root@dakube:~#
root@dakube:~# docker ps
CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES
root@dakube:~#
root@dakube:~#
root@dakube:~# lsns
        NS TYPE   NPROCS   PID USER             COMMAND
4026531834 time      147     1 root             /sbin/init
4026531835 cgroup    144     1 root             /sbin/init
4026531836 pid       141     1 root             /sbin/init
4026531837 user      147     1 root             /sbin/init
4026531838 uts       138     1 root             /sbin/init
4026531839 ipc       141     1 root             /sbin/init
4026531840 net       141     1 root             /sbin/init
4026531841 mnt       137     1 root             /sbin/init
4026532207 mnt         1   255 root             |-/lib/systemd/systemd-udevd
4026532208 uts         1   255 root             |-/lib/systemd/systemd-udevd
4026532468 mnt         1   460 systemd-timesync |-/lib/systemd/systemd-timesyncd
4026532472 uts         1   460 systemd-timesync |-/lib/systemd/systemd-timesyncd
4026532547 mnt         1   519 root             |-/lib/systemd/systemd-logind
4026532548 uts         1   519 root             `-/lib/systemd/systemd-logind
4026531862 mnt         1    34 root             kdevtmpfs
4026532509 net         2  1730 65535            /pause
4026532608 net         2  1737 65535            /pause
4026532692 net         2  1794 65535            /pause
4026532773 mnt         1  1730 65535            /pause
4026532774 uts         2  1730 65535            /pause
4026532775 ipc         2  1730 65535            /pause
4026532776 pid         1  1730 65535            /pause
4026532777 mnt         1  1737 65535            /pause
4026532778 uts         2  1737 65535            /pause
4026532779 ipc         2  1737 65535            /pause
4026532780 pid         1  1737 65535            /pause
4026532781 mnt         1  1794 65535            /pause
4026532782 uts         2  1794 65535            /pause
4026532783 ipc         2  1794 65535            /pause
4026532784 pid         1  1794 65535            /pause
4026532785 mnt         1  1857 root             local-path-provisioner start --config /etc/config/config.json
4026532786 pid         1  1857 root             local-path-provisioner start --config /etc/config/config.json
4026532787 cgroup      1  1857 root             local-path-provisioner start --config /etc/config/config.json
4026532788 mnt         1  1865 65532            /coredns -conf /etc/coredns/Corefile
4026532789 pid         1  1865 65532            /coredns -conf /etc/coredns/Corefile
4026532790 cgroup      1  1865 65532            /coredns -conf /etc/coredns/Corefile
4026532791 mnt         1  1898 rouls            /metrics-server --cert-dir=/tmp --secure-port=10250 --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --kubelet-use-node-status-port --metric-reso
4026532792 pid         1  1898 rouls            /metrics-server --cert-dir=/tmp --secure-port=10250 --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --kubelet-use-node-status-port --metric-reso
4026532793 cgroup      1  1898 rouls            /metrics-server --cert-dir=/tmp --secure-port=10250 --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --kubelet-use-node-status-port --metric-reso
root@dakube:~# lsns |grep test
root@dakube:~# docker run -d --name test alpine sleep 300
docker: Error response from daemon: Conflict. The container name "/test" is already in use by container "3c93c30ffd2aed33036df2c213ebc38b2f7e14a7aa478bed3182bec7505fe4d2". You have to remove (or rename) that container to be able to reuse that name.

Run 'docker run --help' for more information
root@dakube:~# lsns |grep test
root@dakube:~# docker ps
CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES
root@dakube:~# docker top ps
Error response from daemon: No such container: ps
root@dakube:~#
root@dakube:~#
root@dakube:~# docker top
docker: 'docker top' requires at least 1 argument

Usage:  docker top CONTAINER [ps OPTIONS]

See 'docker top --help' for more information
root@dakube:~# pid=$(docker inspect -f '{{.State.Pid}}' test)
root@dakube:~# lsns -p $pid
        NS TYPE   NPROCS   PID USER             COMMAND
4026531834 time      146     1 root             /sbin/init
4026531835 cgroup    143     1 root             /sbin/init
4026531836 pid       140     1 root             /sbin/init
4026531837 user      146     1 root             /sbin/init
4026531838 uts       137     1 root             /sbin/init
4026531839 ipc       140     1 root             /sbin/init
4026531840 net       140     1 root             /sbin/init
4026531841 mnt       136     1 root             /sbin/init
4026532207 mnt         1   255 root             |-/lib/systemd/systemd-udevd
4026532208 uts         1   255 root             |-/lib/systemd/systemd-udevd
4026532468 mnt         1   460 systemd-timesync |-/lib/systemd/systemd-timesyncd
4026532472 uts         1   460 systemd-timesync |-/lib/systemd/systemd-timesyncd
4026532547 mnt         1   519 root             |-/lib/systemd/systemd-logind
4026532548 uts         1   519 root             `-/lib/systemd/systemd-logind
4026531862 mnt         1    34 root             kdevtmpfs
4026532509 net         2  1730 65535            /pause
4026532608 net         2  1737 65535            /pause
4026532692 net         2  1794 65535            /pause
4026532773 mnt         1  1730 65535            /pause
4026532774 uts         2  1730 65535            /pause
4026532775 ipc         2  1730 65535            /pause
4026532776 pid         1  1730 65535            /pause
4026532777 mnt         1  1737 65535            /pause
4026532778 uts         2  1737 65535            /pause
4026532779 ipc         2  1737 65535            /pause
4026532780 pid         1  1737 65535            /pause
4026532781 mnt         1  1794 65535            /pause
4026532782 uts         2  1794 65535            /pause
4026532783 ipc         2  1794 65535            /pause
4026532784 pid         1  1794 65535            /pause
4026532785 mnt         1  1857 root             local-path-provisioner start --config /etc/config/config.json
4026532786 pid         1  1857 root             local-path-provisioner start --config /etc/config/config.json
4026532787 cgroup      1  1857 root             local-path-provisioner start --config /etc/config/config.json
4026532788 mnt         1  1865 65532            /coredns -conf /etc/coredns/Corefile
4026532789 pid         1  1865 65532            /coredns -conf /etc/coredns/Corefile
4026532790 cgroup      1  1865 65532            /coredns -conf /etc/coredns/Corefile
4026532791 mnt         1  1898 rouls            /metrics-server --cert-dir=/tmp --secure-port=10250 --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --kubelet-use-node-status-port --metric-reso
4026532792 pid         1  1898 rouls            /metrics-server --cert-dir=/tmp --secure-port=10250 --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --kubelet-use-node-status-port --metric-reso
4026532793 cgroup      1  1898 rouls            /metrics-server --cert-dir=/tmp --secure-port=10250 --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --kubelet-use-node-status-port --metric-reso
root@dakube:~#

# docker run -d --name test alpine sleep 300
bf5bc826b382f0138b98d639d728258357513b2c07d6bc5e30f95fb97c327b29
root@dakube:~#
root@dakube:~# docker exec -it test sh
/ # touch /hello-from-container
/ # exit
root@dakube:~# ls /hello-from-container
/hello-from-container
root@dakube:~# ls /
bin  boot  dev  etc  hello-from-container  home  initrd.img  initrd.img.old  lib  lost+found  media  mnt  opt  proc  root  run  sbin  srv  swapfile  sys  tmp  usr  var  vmlinuz  vmlinuz.old
root@dakube:~# docker rm -f test 2>/dev/null
test
root@dakube:~# rm -f /hello-from-container
root@dakube:~# ls /hello-from-container
ls: cannot access '/hello-from-container': No such file or directory
root@dakube:~# docker run -d --name test alpine sleep 300
94d56d10bafe7212ff8855a54144c79d48be6aee939fef62b192d387064406b6
root@dakube:~# docker exec -it test sh
/ # touch /hello-from-container
/ # ls /
bin                   etc                   home                  media                 opt                   root                  sbin                  sys                   usr
dev                   hello-from-container  lib                   mnt                   proc                  run                   srv                   tmp                   var
/ # exit
root@dakube:~# ls /hello-from-container
ls: cannot access '/hello-from-container': No such file or directory
root@dakube:~#
root@dakube:~# docker inspect test | grep UpperDir
                "UpperDir": "/var/lib/docker/overlay2/11b89d60a2785bf53443b666bf162d554b06df4134885fc54de02f978eaf5c71/diff",
root@dakube:~# ls /var/lib/docker/overlay2/11b89d60a2785bf53443b666bf162d554b06df4134885fc54de02f978eaf5c71/diff
hello-from-container  root
root@dakube:~#

root@dakube:~# ip addr show docker0
4: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default
    link/ether 02:42:d9:00:ff:3c brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:d9ff:fe00:ff3c/64 scope link
       valid_lft forever preferred_lft forever
root@dakube:~#
root@dakube:~#
root@dakube:~# ip route | grep docker0
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1
root@dakube:~# docker run -d --name nettest alpine sleep 600
47a619dd88cf1a62619773fca66fc2658df03345f930fc972e4c3876912a87c4
root@dakube:~# docker inspect nettest | grep -A 5 NetworkSettings
        "NetworkSettings": {
            "SandboxID": "08bf462e5c3d6e02a5e16e40749dccdd15eeffe57ffc1448ac87b18c944050da",
            "SandboxKey": "/var/run/docker/netns/08bf462e5c3d",
            "Ports": {},
            "Networks": {
                "bridge": {
root@dakube:~# docker inspect nettest | grep 172.17
                    "Gateway": "172.17.0.1",
                    "IPAddress": "172.17.0.3",
root@dakube:~# ip link|grep veth
7: vethdab01af8@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP mode DEFAULT group default qlen 1000
8: veth754519be@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP mode DEFAULT group default qlen 1000
9: vethcefaeded@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP mode DEFAULT group default qlen 1000
20: vethe804761@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default
21: vethd9af464@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default
root@dakube:~# docker exec nettest ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0@if21: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP
    link/ether 0e:6f:a8:27:48:25 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.3/16 brd 172.17.255.255 scope global eth0
       valid_lft forever preferred_lft forever
root@dakube:~# ping -c 3 172.17.0.2
PING 172.17.0.2 (172.17.0.2) 56(84) bytes of data.
From 172.17.0.1 icmp_seq=1 Destination Host Unreachable
From 172.17.0.1 icmp_seq=2 Destination Host Unreachable
From 172.17.0.1 icmp_seq=3 Destination Host Unreachable

--- 172.17.0.2 ping statistics ---
3 packets transmitted, 0 received, +3 errors, 100% packet loss, time 2055ms
pipe 3
root@dakube:~# ping -c 3 172.17.0.2cc^C
root@dakube:~# docker exec -it nettest sh
/ # ping -c 3 172.17.0.1
PING 172.17.0.1 (172.17.0.1): 56 data bytes
64 bytes from 172.17.0.1: seq=0 ttl=64 time=0.453 ms
64 bytes from 172.17.0.1: seq=1 ttl=64 time=0.310 ms
^C
--- 172.17.0.1 ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max = 0.310/0.381/0.453 ms
/ # exit
root@dakube:~# docker exec -it nettest sh
/ # apk add --no-cache curl
( 1/10) Installing brotli-libs (1.2.0-r0)
( 2/10) Installing c-ares (1.34.6-r0)
( 3/10) Installing libunistring (1.4.1-r0)
( 4/10) Installing libidn2 (2.3.8-r0)
( 5/10) Installing nghttp2-libs (1.68.0-r0)
( 6/10) Installing nghttp3 (1.13.1-r0)
( 7/10) Installing libpsl (0.21.5-r3)
( 8/10) Installing zstd-libs (1.5.7-r2)
( 9/10) Installing libcurl (8.17.0-r1)
(10/10) Installing curl (8.17.0-r1)
Executing busybox-1.37.0-r30.trigger
OK: 13.6 MiB in 26 packages
/ # curl -I https://google.com
HTTP/2 301
location: https://www.google.com/
content-type: text/html; charset=UTF-8
content-security-policy-report-only: object-src 'none';base-uri 'self';script-src 'nonce-TQfETIlCxyAltdcRV49-6A' 'strict-dynamic' 'report-sample' 'unsafe-eval' 'unsafe-inline' https: http:;report-uri https://csp.withgoogle.com/csp/gws/other-hp
date: Sat, 10 Jan 2026 21:57:13 GMT
expires: Mon, 09 Feb 2026 21:57:13 GMT
cache-control: public, max-age=2592000
server: gws
content-length: 220
x-xss-protection: 0
x-frame-options: SAMEORIGIN
alt-svc: h3=":443"; ma=2592000,h3-29=":443"; ma=2592000

/ # exit
root@dakube:~# iptables -t nat -L -n -v | grep docker
    4   267 MASQUERADE  0    --  *      !docker0  172.17.0.0/16        0.0.0.0/0


TP 3.5

# mount |grep cgroup
cgroup2 on /sys/fs/cgroup type cgroup2 (rw,nosuid,nodev,noexec,relatime,nsdelegate,memory_recursiveprot)
root@dakube:~# docker run -d --name stress alpine sleep 600
19737b43efc314910772a54c2b2b6310604cac611d76d15d8df919677a52b447
root@dakube:~# docker exec -it stress sh
/ # apk add --no-cache stress-ng
(1/5) Installing judy (1.0.5-r1)
(2/5) Installing libmd (1.1.0-r0)
(3/5) Installing libbsd (0.12.2-r0)
(4/5) Installing liblksctp (1.0.19-r5)
(5/5) Installing stress-ng (0.19.03-r0)
Executing busybox-1.37.0-r30.trigger
OK: 13.4 MiB in 21 packages
/ # exit
root@dakube:~# docker exec stress stress-ng --cpu 1 --timeout 30s
stress-ng: info:  [16] setting to a 30 secs run per stressor
stress-ng: info:  [16] dispatching hogs: 1 cpu

stress-ng: info:  [16] skipped: 0
stress-ng: info:  [16] passed: 1: cpu (1)
stress-ng: info:  [16] failed: 0
stress-ng: info:  [16] metrics untrustworthy: 0
stress-ng: info:  [16] successful run completed in 30.13 secs
root@dakube:~#
root@dakube:~# docker exec stress stress-ng --cpu 1 --timeout 30s
stress-ng: info:  [23] setting to a 30 secs run per stressor
stress-ng: info:  [23] dispatching hogs: 1 cpu
stress-ng: info:  [23] skipped: 0
stress-ng: info:  [23] passed: 1: cpu (1)
stress-ng: info:  [23] failed: 0
stress-ng: info:  [23] metrics untrustworthy: 0
stress-ng: info:  [23] successful run completed in 30.06 secs
root@dakube:~# docker rm -f stress
stress
root@dakube:~# docker run -d \
  --name stress \
  --cpus="0.5" \
  alpine sleep 600
8b76b82f2d27f2d32d78003f7ea992a167c531000c4e6a92d0a72cbea41da326
root@dakube:~#
root@dakube:~#
root@dakube:~# docker ps
CONTAINER ID   IMAGE     COMMAND       CREATED         STATUS         PORTS     NAMES
8b76b82f2d27   alpine    "sleep 600"   6 seconds ago   Up 6 seconds             stress
root@dakube:~#
root@dakube:~#
root@dakube:~# docker exec -it stress sh
/ # apk add --no-cache stress-ng
(1/5) Installing judy (1.0.5-r1)
(2/5) Installing libmd (1.1.0-r0)
(3/5) Installing libbsd (0.12.2-r0)
(4/5) Installing liblksctp (1.0.19-r5)
(5/5) Installing stress-ng (0.19.03-r0)
Executing busybox-1.37.0-r30.trigger
OK: 13.4 MiB in 21 packages
/ # stress-ng --cpu 1 --timeout 30s
stress-ng: info:  [16] setting to a 30 secs run per stressor
stress-ng: info:  [16] dispatching hogs: 1 cpu
stress-ng: info:  [16] skipped: 0
stress-ng: info:  [16] passed: 1: cpu (1)
stress-ng: info:  [16] failed: 0
stress-ng: info:  [16] metrics untrustworthy: 0
stress-ng: info:  [16] successful run completed in 30.05 secs
/ # exit
root@dakube:~# docker inspect --format '{{.Id}}' stress
8b76b82f2d27f2d32d78003f7ea992a167c531000c4e6a92d0a72cbea41da326
root@dakube:~# cd /sys/fs/cgroup
grep stress -R .
grep: ./sys-fs-fuse-connections.mount/cgroup.kill: Invalid argument
grep: ./sys-fs-fuse-connections.mount/memory.reclaim: Invalid argument
grep: ./sys-kernel-config.mount/cgroup.kill: Invalid argument
grep: ./sys-kernel-config.mount/memory.reclaim: Invalid argument
grep: ./sys-kernel-debug.mount/cgroup.kill: Invalid argument
grep: ./sys-kernel-debug.mount/memory.reclaim: Invalid argument
grep: ./dev-mqueue.mount/cgroup.kill: Invalid argument
grep: ./dev-mqueue.mount/memory.reclaim: Invalid argument
grep: ./user.slice/cgroup.kill: Invalid argument
grep: ./user.slice/user-1000.slice/user@1000.service/cgroup.kill: Invalid argument
grep: ./user.slice/user-1000.slice/user@1000.service/app.slice/dbus.socket/cgroup.kill: Invalid argument
grep: ./user.slice/user-1000.slice/user@1000.service/app.slice/dbus.socket/memory.reclaim: Invalid argument
grep: ./user.slice/user-1000.slice/user@1000.service/app.slice/cgroup.kill: Invalid argument
grep: ./user.slice/user-1000.slice/user@1000.service/app.slice/memory.reclaim: Invalid argument
grep: ./user.slice/user-1000.slice/user@1000.service/init.scope/cgroup.kill: Invalid argument
grep: ./user.slice/user-1000.slice/user@1000.service/init.scope/memory.reclaim: Invalid argument
grep: ./user.slice/user-1000.slice/user@1000.service/memory.reclaim: Invalid argument
grep: ./user.slice/user-1000.slice/session-3.scope/cgroup.kill: Invalid argument
grep: ./user.slice/user-1000.slice/session-3.scope/memory.reclaim: Invalid argument
grep: ./user.slice/user-1000.slice/cgroup.kill: Invalid argument
grep: ./user.slice/user-1000.slice/session-1.scope/cgroup.kill: Invalid argument
grep: ./user.slice/user-1000.slice/session-1.scope/memory.reclaim: Invalid argument
grep: ./user.slice/user-1000.slice/memory.reclaim: Invalid argument
grep: ./user.slice/memory.reclaim: Invalid argument
grep: ./sys-kernel-tracing.mount/cgroup.kill: Invalid argument
grep: ./sys-kernel-tracing.mount/memory.reclaim: Invalid argument
grep: ./init.scope/cgroup.kill: Invalid argument
grep: ./init.scope/memory.reclaim: Invalid argument
grep: ./system.slice/system-systemd\x2dfsck.slice/cgroup.kill: Invalid argument
grep: ./system.slice/system-systemd\x2dfsck.slice/memory.reclaim: Invalid argument
grep: ./system.slice/containerd.service/cgroup.kill: Invalid argument
grep: ./system.slice/containerd.service/memory.reclaim: Invalid argument
grep: ./system.slice/systemd-udevd.service/cgroup.kill: Invalid argument
grep: ./system.slice/systemd-udevd.service/udev/cgroup.kill: Invalid argument
grep: ./system.slice/systemd-udevd.service/memory.reclaim: Invalid argument
grep: ./system.slice/docker-8b76b82f2d27f2d32d78003f7ea992a167c531000c4e6a92d0a72cbea41da326.scope/cgroup.kill: Invalid argument
grep: ./system.slice/docker-8b76b82f2d27f2d32d78003f7ea992a167c531000c4e6a92d0a72cbea41da326.scope/memory.reclaim: Invalid argument
grep: ./system.slice/cron.service/cgroup.kill: Invalid argument
grep: ./system.slice/cron.service/memory.reclaim: Invalid argument
grep: ./system.slice/system-serial\x2dgetty.slice/serial-getty@ttyS1.service/cgroup.kill: Invalid argument
grep: ./system.slice/system-serial\x2dgetty.slice/serial-getty@ttyS1.service/memory.reclaim: Invalid argument
grep: ./system.slice/system-serial\x2dgetty.slice/cgroup.kill: Invalid argument
grep: ./system.slice/system-serial\x2dgetty.slice/memory.reclaim: Invalid argument
grep: ./system.slice/networking.service/cgroup.kill: Invalid argument
grep: ./system.slice/networking.service/memory.reclaim: Invalid argument
grep: ./system.slice/docker.service/cgroup.kill: Invalid argument
grep: ./system.slice/docker.service/memory.reclaim: Invalid argument
grep: ./system.slice/rpcbind.socket/cgroup.kill: Invalid argument
grep: ./system.slice/rpcbind.socket/memory.reclaim: Invalid argument
grep: ./system.slice/system-modprobe.slice/cgroup.kill: Invalid argument
grep: ./system.slice/system-modprobe.slice/memory.reclaim: Invalid argument
grep: ./system.slice/systemd-journald.service/cgroup.kill: Invalid argument
grep: ./system.slice/systemd-journald.service/memory.reclaim: Invalid argument
grep: ./system.slice/ssh.service/cgroup.kill: Invalid argument
grep: ./system.slice/ssh.service/memory.reclaim: Invalid argument
grep: ./system.slice/boot-firmware.mount/cgroup.kill: Invalid argument
grep: ./system.slice/boot-firmware.mount/memory.reclaim: Invalid argument
grep: ./system.slice/cgroup.kill: Invalid argument
grep: ./system.slice/rpcbind.service/cgroup.kill: Invalid argument
grep: ./system.slice/rpcbind.service/memory.reclaim: Invalid argument
grep: ./system.slice/k3s.service/cgroup.kill: Invalid argument
grep: ./system.slice/k3s.service/memory.reclaim: Invalid argument
grep: ./system.slice/run-rpc_pipefs.mount/cgroup.kill: Invalid argument
grep: ./system.slice/run-rpc_pipefs.mount/memory.reclaim: Invalid argument
grep: ./system.slice/docker.socket/cgroup.kill: Invalid argument
grep: ./system.slice/docker.socket/memory.reclaim: Invalid argument
grep: ./system.slice/mnt-nfs-k3s.mount/cgroup.kill: Invalid argument
grep: ./system.slice/mnt-nfs-k3s.mount/memory.reclaim: Invalid argument
grep: ./system.slice/dbus.service/cgroup.kill: Invalid argument
grep: ./system.slice/dbus.service/memory.reclaim: Invalid argument
grep: ./system.slice/systemd-timesyncd.service/cgroup.kill: Invalid argument
grep: ./system.slice/systemd-timesyncd.service/memory.reclaim: Invalid argument
grep: ./system.slice/memory.reclaim: Invalid argument
grep: ./system.slice/system-getty.slice/getty@tty1.service/cgroup.kill: Invalid argument
grep: ./system.slice/system-getty.slice/getty@tty1.service/memory.reclaim: Invalid argument
grep: ./system.slice/system-getty.slice/cgroup.kill: Invalid argument
grep: ./system.slice/system-getty.slice/memory.reclaim: Invalid argument
grep: ./system.slice/systemd-logind.service/cgroup.kill: Invalid argument
grep: ./system.slice/systemd-logind.service/memory.reclaim: Invalid argument
grep: ./proc-sys-fs-binfmt_misc.mount/cgroup.kill: Invalid argument
grep: ./proc-sys-fs-binfmt_misc.mount/memory.reclaim: Invalid argument
grep: ./dev-hugepages.mount/cgroup.kill: Invalid argument
grep: ./dev-hugepages.mount/memory.reclaim: Invalid argument
grep: ./memory.reclaim: Invalid argument
grep: ./kubepods.slice/cgroup.kill: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod2c5d91bf_4a9e_46f3_9cca_e10eda2d90e1.slice/cri-containerd-c87d14c58072fb44a1934961a1a8d31238fe10c266ae89609e0a852c2002fcd8.scope/cgroup.kill: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod2c5d91bf_4a9e_46f3_9cca_e10eda2d90e1.slice/cri-containerd-c87d14c58072fb44a1934961a1a8d31238fe10c266ae89609e0a852c2002fcd8.scope/memory.reclaim: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod2c5d91bf_4a9e_46f3_9cca_e10eda2d90e1.slice/cgroup.kill: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod2c5d91bf_4a9e_46f3_9cca_e10eda2d90e1.slice/cri-containerd-9e91a6f0317dba25e79f77099f0583968fbcc7887d9611ff8525c899cdb370a4.scope/cgroup.kill: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod2c5d91bf_4a9e_46f3_9cca_e10eda2d90e1.slice/cri-containerd-9e91a6f0317dba25e79f77099f0583968fbcc7887d9611ff8525c899cdb370a4.scope/memory.reclaim: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod2c5d91bf_4a9e_46f3_9cca_e10eda2d90e1.slice/memory.reclaim: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/cgroup.kill: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podb1108a4a_eb98_4ca7_be9d_779ddcb95c8a.slice/cgroup.kill: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podb1108a4a_eb98_4ca7_be9d_779ddcb95c8a.slice/cri-containerd-6233238dd0dd37f71668189af309ea791bc8777b3218351b748752944aa5b0d8.scope/cgroup.kill: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podb1108a4a_eb98_4ca7_be9d_779ddcb95c8a.slice/cri-containerd-6233238dd0dd37f71668189af309ea791bc8777b3218351b748752944aa5b0d8.scope/memory.reclaim: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podb1108a4a_eb98_4ca7_be9d_779ddcb95c8a.slice/cri-containerd-4f729cfd32f2109d8c225ac4b924b6a07bcd53ddf55d089230a4ab24e7336e7a.scope/cgroup.kill: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podb1108a4a_eb98_4ca7_be9d_779ddcb95c8a.slice/cri-containerd-4f729cfd32f2109d8c225ac4b924b6a07bcd53ddf55d089230a4ab24e7336e7a.scope/memory.reclaim: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podb1108a4a_eb98_4ca7_be9d_779ddcb95c8a.slice/memory.reclaim: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/memory.reclaim: Invalid argument
grep: ./kubepods.slice/kubepods-besteffort.slice/cgroup.kill: Invalid argument
grep: ./kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod7a21603a_a8bd_4827_b9de_0871e1c90f14.slice/cri-containerd-2f3d25a55591f94349b60f674b5da036620d8d962e8590612581028ba75852c2.scope/cgroup.kill: Invalid argument
grep: ./kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod7a21603a_a8bd_4827_b9de_0871e1c90f14.slice/cri-containerd-2f3d25a55591f94349b60f674b5da036620d8d962e8590612581028ba75852c2.scope/memory.reclaim: Invalid argument
grep: ./kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod7a21603a_a8bd_4827_b9de_0871e1c90f14.slice/cgroup.kill: Invalid argument
grep: ./kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod7a21603a_a8bd_4827_b9de_0871e1c90f14.slice/cri-containerd-a1b6d7811b5d346578b04ea88f183830d8c7324856e686a2fa5d15723fb3b801.scope/cgroup.kill: Invalid argument
grep: ./kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod7a21603a_a8bd_4827_b9de_0871e1c90f14.slice/cri-containerd-a1b6d7811b5d346578b04ea88f183830d8c7324856e686a2fa5d15723fb3b801.scope/memory.reclaim: Invalid argument
grep: ./kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod7a21603a_a8bd_4827_b9de_0871e1c90f14.slice/memory.reclaim: Invalid argument
grep: ./kubepods.slice/kubepods-besteffort.slice/memory.reclaim: Invalid argument
grep: ./kubepods.slice/memory.reclaim: Invalid argument
root@dakube:/sys/fs/cgroup# docker ps
CONTAINER ID   IMAGE     COMMAND       CREATED              STATUS              PORTS     NAMES
8b76b82f2d27   alpine    "sleep 600"   About a minute ago   Up About a minute             stress
root@dakube:/sys/fs/cgroup# docker inspect --format '{{.8b76b82f2d27}}' stress
template parsing error: template: :1: bad number syntax: ".8b"
root@dakube:/sys/fs/cgroup# docker inspect --format '{{8b76b82f2d27}}' stress
template parsing error: template: :1: bad number syntax: "8b"
root@dakube:/sys/fs/cgroup# docker inspect --format '8b76b82f2d27' stress
8b76b82f2d27
root@dakube:/sys/fs/cgroup# docker ps
CONTAINER ID   IMAGE     COMMAND       CREATED         STATUS         PORTS     NAMES
8b76b82f2d27   alpine    "sleep 600"   3 minutes ago   Up 3 minutes             stress
root@dakube:/sys/fs/cgroup# docker ps
CONTAINER ID   IMAGE     COMMAND       CREATED         STATUS         PORTS     NAMES
8b76b82f2d27   alpine    "sleep 600"   3 minutes ago   Up 3 minutes             stress
root@dakube:/sys/fs/cgroup#
root@dakube:/sys/fs/cgroup#
root@dakube:/sys/fs/cgroup# grep stress -R .
grep: ./sys-fs-fuse-connections.mount/cgroup.kill: Invalid argument
grep: ./sys-fs-fuse-connections.mount/memory.reclaim: Invalid argument
grep: ./sys-kernel-config.mount/cgroup.kill: Invalid argument
grep: ./sys-kernel-config.mount/memory.reclaim: Invalid argument
grep: ./sys-kernel-debug.mount/cgroup.kill: Invalid argument
grep: ./sys-kernel-debug.mount/memory.reclaim: Invalid argument
grep: ./dev-mqueue.mount/cgroup.kill: Invalid argument
grep: ./dev-mqueue.mount/memory.reclaim: Invalid argument
grep: ./user.slice/cgroup.kill: Invalid argument
grep: ./user.slice/user-1000.slice/user@1000.service/cgroup.kill: Invalid argument
grep: ./user.slice/user-1000.slice/user@1000.service/app.slice/dbus.socket/cgroup.kill: Invalid argument
grep: ./user.slice/user-1000.slice/user@1000.service/app.slice/dbus.socket/memory.reclaim: Invalid argument
grep: ./user.slice/user-1000.slice/user@1000.service/app.slice/cgroup.kill: Invalid argument
grep: ./user.slice/user-1000.slice/user@1000.service/app.slice/memory.reclaim: Invalid argument
grep: ./user.slice/user-1000.slice/user@1000.service/init.scope/cgroup.kill: Invalid argument
grep: ./user.slice/user-1000.slice/user@1000.service/init.scope/memory.reclaim: Invalid argument
grep: ./user.slice/user-1000.slice/user@1000.service/memory.reclaim: Invalid argument
grep: ./user.slice/user-1000.slice/session-3.scope/cgroup.kill: Invalid argument
grep: ./user.slice/user-1000.slice/session-3.scope/memory.reclaim: Invalid argument
grep: ./user.slice/user-1000.slice/cgroup.kill: Invalid argument
grep: ./user.slice/user-1000.slice/session-1.scope/cgroup.kill: Invalid argument
grep: ./user.slice/user-1000.slice/session-1.scope/memory.reclaim: Invalid argument
grep: ./user.slice/user-1000.slice/memory.reclaim: Invalid argument
grep: ./user.slice/memory.reclaim: Invalid argument
grep: ./sys-kernel-tracing.mount/cgroup.kill: Invalid argument
grep: ./sys-kernel-tracing.mount/memory.reclaim: Invalid argument
grep: ./init.scope/cgroup.kill: Invalid argument
grep: ./init.scope/memory.reclaim: Invalid argument
grep: ./system.slice/system-systemd\x2dfsck.slice/cgroup.kill: Invalid argument
grep: ./system.slice/system-systemd\x2dfsck.slice/memory.reclaim: Invalid argument
grep: ./system.slice/containerd.service/cgroup.kill: Invalid argument
grep: ./system.slice/containerd.service/memory.reclaim: Invalid argument
grep: ./system.slice/systemd-udevd.service/cgroup.kill: Invalid argument
grep: ./system.slice/systemd-udevd.service/udev/cgroup.kill: Invalid argument
grep: ./system.slice/systemd-udevd.service/memory.reclaim: Invalid argument
grep: ./system.slice/docker-8b76b82f2d27f2d32d78003f7ea992a167c531000c4e6a92d0a72cbea41da326.scope/cgroup.kill: Invalid argument
grep: ./system.slice/docker-8b76b82f2d27f2d32d78003f7ea992a167c531000c4e6a92d0a72cbea41da326.scope/memory.reclaim: Invalid argument
grep: ./system.slice/cron.service/cgroup.kill: Invalid argument
grep: ./system.slice/cron.service/memory.reclaim: Invalid argument
grep: ./system.slice/system-serial\x2dgetty.slice/serial-getty@ttyS1.service/cgroup.kill: Invalid argument
grep: ./system.slice/system-serial\x2dgetty.slice/serial-getty@ttyS1.service/memory.reclaim: Invalid argument
grep: ./system.slice/system-serial\x2dgetty.slice/cgroup.kill: Invalid argument
grep: ./system.slice/system-serial\x2dgetty.slice/memory.reclaim: Invalid argument
grep: ./system.slice/networking.service/cgroup.kill: Invalid argument
grep: ./system.slice/networking.service/memory.reclaim: Invalid argument
grep: ./system.slice/docker.service/cgroup.kill: Invalid argument
grep: ./system.slice/docker.service/memory.reclaim: Invalid argument
grep: ./system.slice/rpcbind.socket/cgroup.kill: Invalid argument
grep: ./system.slice/rpcbind.socket/memory.reclaim: Invalid argument
grep: ./system.slice/system-modprobe.slice/cgroup.kill: Invalid argument
grep: ./system.slice/system-modprobe.slice/memory.reclaim: Invalid argument
grep: ./system.slice/systemd-journald.service/cgroup.kill: Invalid argument
grep: ./system.slice/systemd-journald.service/memory.reclaim: Invalid argument
grep: ./system.slice/ssh.service/cgroup.kill: Invalid argument
grep: ./system.slice/ssh.service/memory.reclaim: Invalid argument
grep: ./system.slice/boot-firmware.mount/cgroup.kill: Invalid argument
grep: ./system.slice/boot-firmware.mount/memory.reclaim: Invalid argument
grep: ./system.slice/cgroup.kill: Invalid argument
grep: ./system.slice/rpcbind.service/cgroup.kill: Invalid argument
grep: ./system.slice/rpcbind.service/memory.reclaim: Invalid argument
grep: ./system.slice/k3s.service/cgroup.kill: Invalid argument
grep: ./system.slice/k3s.service/memory.reclaim: Invalid argument
grep: ./system.slice/run-rpc_pipefs.mount/cgroup.kill: Invalid argument
grep: ./system.slice/run-rpc_pipefs.mount/memory.reclaim: Invalid argument
grep: ./system.slice/docker.socket/cgroup.kill: Invalid argument
grep: ./system.slice/docker.socket/memory.reclaim: Invalid argument
grep: ./system.slice/mnt-nfs-k3s.mount/cgroup.kill: Invalid argument
grep: ./system.slice/mnt-nfs-k3s.mount/memory.reclaim: Invalid argument
grep: ./system.slice/dbus.service/cgroup.kill: Invalid argument
grep: ./system.slice/dbus.service/memory.reclaim: Invalid argument
grep: ./system.slice/systemd-timesyncd.service/cgroup.kill: Invalid argument
grep: ./system.slice/systemd-timesyncd.service/memory.reclaim: Invalid argument
grep: ./system.slice/memory.reclaim: Invalid argument
grep: ./system.slice/system-getty.slice/getty@tty1.service/cgroup.kill: Invalid argument
grep: ./system.slice/system-getty.slice/getty@tty1.service/memory.reclaim: Invalid argument
grep: ./system.slice/system-getty.slice/cgroup.kill: Invalid argument
grep: ./system.slice/system-getty.slice/memory.reclaim: Invalid argument
grep: ./system.slice/systemd-logind.service/cgroup.kill: Invalid argument
grep: ./system.slice/systemd-logind.service/memory.reclaim: Invalid argument
grep: ./proc-sys-fs-binfmt_misc.mount/cgroup.kill: Invalid argument
grep: ./proc-sys-fs-binfmt_misc.mount/memory.reclaim: Invalid argument
grep: ./dev-hugepages.mount/cgroup.kill: Invalid argument
grep: ./dev-hugepages.mount/memory.reclaim: Invalid argument
grep: ./memory.reclaim: Invalid argument
grep: ./kubepods.slice/cgroup.kill: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod2c5d91bf_4a9e_46f3_9cca_e10eda2d90e1.slice/cri-containerd-c87d14c58072fb44a1934961a1a8d31238fe10c266ae89609e0a852c2002fcd8.scope/cgroup.kill: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod2c5d91bf_4a9e_46f3_9cca_e10eda2d90e1.slice/cri-containerd-c87d14c58072fb44a1934961a1a8d31238fe10c266ae89609e0a852c2002fcd8.scope/memory.reclaim: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod2c5d91bf_4a9e_46f3_9cca_e10eda2d90e1.slice/cgroup.kill: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod2c5d91bf_4a9e_46f3_9cca_e10eda2d90e1.slice/cri-containerd-9e91a6f0317dba25e79f77099f0583968fbcc7887d9611ff8525c899cdb370a4.scope/cgroup.kill: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod2c5d91bf_4a9e_46f3_9cca_e10eda2d90e1.slice/cri-containerd-9e91a6f0317dba25e79f77099f0583968fbcc7887d9611ff8525c899cdb370a4.scope/memory.reclaim: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod2c5d91bf_4a9e_46f3_9cca_e10eda2d90e1.slice/memory.reclaim: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/cgroup.kill: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podb1108a4a_eb98_4ca7_be9d_779ddcb95c8a.slice/cgroup.kill: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podb1108a4a_eb98_4ca7_be9d_779ddcb95c8a.slice/cri-containerd-6233238dd0dd37f71668189af309ea791bc8777b3218351b748752944aa5b0d8.scope/cgroup.kill: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podb1108a4a_eb98_4ca7_be9d_779ddcb95c8a.slice/cri-containerd-6233238dd0dd37f71668189af309ea791bc8777b3218351b748752944aa5b0d8.scope/memory.reclaim: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podb1108a4a_eb98_4ca7_be9d_779ddcb95c8a.slice/cri-containerd-4f729cfd32f2109d8c225ac4b924b6a07bcd53ddf55d089230a4ab24e7336e7a.scope/cgroup.kill: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podb1108a4a_eb98_4ca7_be9d_779ddcb95c8a.slice/cri-containerd-4f729cfd32f2109d8c225ac4b924b6a07bcd53ddf55d089230a4ab24e7336e7a.scope/memory.reclaim: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podb1108a4a_eb98_4ca7_be9d_779ddcb95c8a.slice/memory.reclaim: Invalid argument
grep: ./kubepods.slice/kubepods-burstable.slice/memory.reclaim: Invalid argument
grep: ./kubepods.slice/kubepods-besteffort.slice/cgroup.kill: Invalid argument
grep: ./kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod7a21603a_a8bd_4827_b9de_0871e1c90f14.slice/cri-containerd-2f3d25a55591f94349b60f674b5da036620d8d962e8590612581028ba75852c2.scope/cgroup.kill: Invalid argument
grep: ./kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod7a21603a_a8bd_4827_b9de_0871e1c90f14.slice/cri-containerd-2f3d25a55591f94349b60f674b5da036620d8d962e8590612581028ba75852c2.scope/memory.reclaim: Invalid argument
grep: ./kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod7a21603a_a8bd_4827_b9de_0871e1c90f14.slice/cgroup.kill: Invalid argument
grep: ./kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod7a21603a_a8bd_4827_b9de_0871e1c90f14.slice/cri-containerd-a1b6d7811b5d346578b04ea88f183830d8c7324856e686a2fa5d15723fb3b801.scope/cgroup.kill: Invalid argument
grep: ./kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod7a21603a_a8bd_4827_b9de_0871e1c90f14.slice/cri-containerd-a1b6d7811b5d346578b04ea88f183830d8c7324856e686a2fa5d15723fb3b801.scope/memory.reclaim: Invalid argument
grep: ./kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod7a21603a_a8bd_4827_b9de_0871e1c90f14.slice/memory.reclaim: Invalid argument
grep: ./kubepods.slice/kubepods-besteffort.slice/memory.reclaim: Invalid argument
grep: ./kubepods.slice/memory.reclaim: Invalid argument
root@dakube:/sys/fs/cgroup# cd system.slice/docker
docker-8b76b82f2d27f2d32d78003f7ea992a167c531000c4e6a92d0a72cbea41da326.scope/
docker.service/
docker.socket/
root@dakube:/sys/fs/cgroup# cd system.slice/docker-8b76b82f2d27f2d32d78003f7ea992a167c531000c4e6a92d0a72cbea41da326.scope/
root@dakube:/sys/fs/cgroup/system.slice/docker-8b76b82f2d27f2d32d78003f7ea992a167c531000c4e6a92d0a72cbea41da326.scope# ls
cgroup.controllers      cpu.max.burst             hugetlb.1GB.numa_stat      hugetlb.32MB.numa_stat     memory.current       memory.swap.events
cgroup.events           cpu.pressure              hugetlb.1GB.rsvd.current   hugetlb.32MB.rsvd.current  memory.events        memory.swap.high
cgroup.freeze           cpu.stat                  hugetlb.1GB.rsvd.max       hugetlb.32MB.rsvd.max      memory.events.local  memory.swap.max
cgroup.kill             cpu.weight                hugetlb.2MB.current        hugetlb.64KB.current       memory.high          memory.zswap.current
cgroup.max.depth        cpu.weight.nice           hugetlb.2MB.events         hugetlb.64KB.events        memory.low           memory.zswap.max
cgroup.max.descendants  cpuset.cpus               hugetlb.2MB.events.local   hugetlb.64KB.events.local  memory.max           misc.current
cgroup.pressure         cpuset.cpus.effective     hugetlb.2MB.max            hugetlb.64KB.max           memory.min           misc.events
cgroup.procs            cpuset.cpus.partition     hugetlb.2MB.numa_stat      hugetlb.64KB.numa_stat     memory.numa_stat     misc.max
cgroup.stat             cpuset.mems               hugetlb.2MB.rsvd.current   hugetlb.64KB.rsvd.current  memory.oom.group     pids.current
cgroup.subtree_control  cpuset.mems.effective     hugetlb.2MB.rsvd.max       hugetlb.64KB.rsvd.max      memory.peak          pids.events
cgroup.threads          hugetlb.1GB.current       hugetlb.32MB.current       io.max                     memory.pressure      pids.max
cgroup.type             hugetlb.1GB.events        hugetlb.32MB.events        io.pressure                memory.reclaim       pids.peak
cpu.idle                hugetlb.1GB.events.local  hugetlb.32MB.events.local  io.stat                    memory.stat          rdma.current
cpu.max                 hugetlb.1GB.max           hugetlb.32MB.max           io.weight                  memory.swap.current  rdma.max
root@dakube:/sys/fs/cgroup/system.slice/docker-8b76b82f2d27f2d32d78003f7ea992a167c531000c4e6a92d0a72cbea41da326.scope# cat cpu.max
50000 100000
root@dakube:/sys/fs/cgroup/system.slice/docker-8b76b82f2d27f2d32d78003f7ea992a167c531000c4e6a92d0a72cbea41da326.scope# cat memory.m
cat: memory.m: No such file or directory
root@dakube:/sys/fs/cgroup/system.slice/docker-8b76b82f2d27f2d32d78003f7ea992a167c531000c4e6a92d0a72cbea41da326.scope# cat memory.max
max
root@dakube:/sys/fs/cgroup/system.slice/docker-8b76b82f2d27f2d32d78003f7ea992a167c531000c4e6a92d0a72cbea41da326.scope# cat memory.current
6328320
root@dakube:/sys/fs/cgroup/system.slice/docker-8b76b82f2d27f2d32d78003f7ea992a167c531000c4e6a92d0a72cbea41da326.scope# docker rm -f stress
stress
root@dakube:/sys/fs/cgroup/system.slice/docker-8b76b82f2d27f2d32d78003f7ea992a167c531000c4e6a92d0a72cbea41da326.scope#
root@dakube:/sys/fs/cgroup/system.slice/docker-8b76b82f2d27f2d32d78003f7ea992a167c531000c4e6a92d0a72cbea41da326.scope# cd
root@dakube:~# docker run -d \
  --name memtest \
  --memory="64m" \
  alpine sleep 600
c3168b57ff5b4d4867a41319123028f98a570b0f67f27ad56889f50df84a685d
root@dakube:~# docker exec -it memtest sh
/ # apk add --no-cache stress-ng
(1/5) Installing judy (1.0.5-r1)
(2/5) Installing libmd (1.1.0-r0)
(3/5) Installing libbsd (0.12.2-r0)
(4/5) Installing liblksctp (1.0.19-r5)
(5/5) Installing stress-ng (0.19.03-r0)
Executing busybox-1.37.0-r30.trigger
OK: 13.4 MiB in 21 packages
/ # stress-ng --vm 1 --vm-bytes 128M --timeout 30s
stress-ng: info:  [16] setting to a 30 secs run per stressor
stress-ng: info:  [16] dispatching hogs: 1 vm
stress-ng: info:  [17] vm: using 128M per stressor instance (total 128M of 556.67M available memory)

stress-ng: warn:  [16] metrics-check: all bogo-op counters are zero, data may be incorrect
stress-ng: info:  [16] skipped: 0
stress-ng: info:  [16] passed: 1: vm (1)
stress-ng: info:  [16] failed: 0
stress-ng: info:  [16] metrics untrustworthy: 0
stress-ng: info:  [16] successful run completed in 30.20 secs
/ #
/ # exit
root@dakube:~# docker ps
CONTAINER ID   IMAGE     COMMAND       CREATED              STATUS              PORTS     NAMES
c3168b57ff5b   alpine    "sleep 600"   About a minute ago   Up About a minute             memtest
root@dakube:~# dmesg |tail
               thp_collapse_alloc 0
[ 1491.051603] Tasks state (memory values in pages):
[ 1491.057130] [  pid  ]   uid  tgid total_vm      rss pgtables_bytes swapents oom_score_adj name
[ 1491.066629] [   3530]     0  3530      417        1    36864        0             0 sleep
[ 1491.075662] [   3581]     0  3581      439      272    36864        0             0 sh
[ 1491.084399] [   3591]     0  3591     3713      590    53248        0             0 stress-ng
[ 1491.093698] [   3592]     0  3592     3714      130    53248        0             0 stress-ng-vm
[ 1491.103232] [   3678]     0  3678    36483    15847   180224        0          1000 stress-ng-vm
[ 1491.112734] oom-kill:constraint=CONSTRAINT_MEMCG,nodemask=(null),cpuset=docker-c3168b57ff5b4d4867a41319123028f98a570b0f67f27ad56889f50df84a685d.scope,mems_allowed=0,oom_memcg=/system.slice/docker-c3168b57ff5b4d4867a41319123028f98a570b0f67f27ad56889f50df84a685d.scope,task_memcg=/system.slice/docker-c3168b57ff5b4d4867a41319123028f98a570b0f67f27ad56889f50df84a685d.scope,task=stress-ng-vm,pid=3678,uid=0
[ 1491.150006] Memory cgroup out of memory: Killed process 3678 (stress-ng-vm) total-vm:145932kB, anon-rss:63004kB, file-rss:324kB, shmem-rss:60kB, UID:0 pgtables:176kB oom_score_adj:1000
root@dakube:~#
root@dakube:~#
root@dakube:~# docker rm -f memtest 2>/dev/null
memtest



TP 4.1
root@dakube:~# systemctl stop docker
Warning: Stopping docker.service, but it can still be activated by:
  docker.socket
root@dakube:~# apt purge -y docker.io docker-ce docker-ce-cli containerd
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following packages were automatically installed and are no longer required:
  cgroupfs-mount criu gettext-base git git-man libcurl3-gnutls liberror-perl libintl-perl libintl-xs-perl libmodule-find-perl libnet1
  libproc-processtable-perl libprotobuf-c1 libprotobuf32 libsort-naturally-perl libterm-readkey-perl needrestart patch pigz python3-protobuf sgml-base
  tini wmdocker
Use 'apt autoremove' to remove them.
The following packages will be REMOVED:
  containerd* docker-ce* docker-ce-cli* docker.io*
0 upgraded, 0 newly installed, 4 to remove and 0 not upgraded.
After this operation, 135 MB disk space will be freed.
(Reading database ... 33812 files and directories currently installed.)
Removing docker-ce (5:29.1.4-1~debian.12~bookworm) ...
Removing docker-ce-cli (5:29.1.4-1~debian.12~bookworm) ...
(Reading database ... 33614 files and directories currently installed.)
Purging configuration files for docker.io (20.10.24+dfsg1-1+deb12u1+b2) ...

Nuking /var/lib/docker ...

+ rm -rf /var/lib/docker/buildkit /var/lib/docker/containers /var/lib/docker/engine-id /var/lib/docker/image /var/lib/docker/network /var/lib/docker/nuke-graph-directory.sh /var/lib/docker/overlay2 /var/lib/docker/plugins /var/lib/docker/runtimes /var/lib/docker/swarm /var/lib/docker/tmp /var/lib/docker/trust /var/lib/docker/volumes
dpkg: warning: while removing docker.io, directory '/etc/docker' not empty so not removed
Purging configuration files for docker-ce (5:29.1.4-1~debian.12~bookworm) ...
Purging configuration files for containerd (1.6.20~ds1-1+deb12u2) ...
root@dakube:~# rm -rf /var/lib/docker /var/lib/containerd
root@dakube:~#
root@dakube:~#
root@dakube:~#
root@dakube:~# ps -ef|grep -i defunct
root        8918    8618  0 13:22 pts/0    00:00:00 grep -i defunct
root@dakube:~# which docker
root@dakube:~# uname -m
aarch64
root@dakube:~# swapon --show
root@dakube:~# curl -sfL https://get.k3s.io | sh -
[INFO]  Finding release for channel stable
[INFO]  Using v1.34.3+k3s1 as release
[INFO]  Downloading hash https://github.com/k3s-io/k3s/releases/download/v1.34.3+k3s1/sha256sum-arm64.txt
[INFO]  Downloading binary https://github.com/k3s-io/k3s/releases/download/v1.34.3+k3s1/k3s-arm64
[INFO]  Verifying binary download
[INFO]  Installing k3s to /usr/local/bin/k3s
[INFO]  Skipping installation of SELinux RPM
[INFO]  Skipping /usr/local/bin/kubectl symlink to k3s, already exists
[INFO]  Skipping /usr/local/bin/crictl symlink to k3s, already exists
[INFO]  Skipping /usr/local/bin/ctr symlink to k3s, already exists
[INFO]  Creating killall script /usr/local/bin/k3s-killall.sh
[INFO]  Creating uninstall script /usr/local/bin/k3s-uninstall.sh
[INFO]  env: Creating environment file /etc/systemd/system/k3s.service.env
[INFO]  systemd: Creating service file /etc/systemd/system/k3s.service
[INFO]  systemd: Enabling k3s unit
Created symlink /etc/systemd/system/multi-user.target.wants/k3s.service → /etc/systemd/system/k3s.service.
[INFO]  systemd: Starting k3s
root@dakube:~#
root@dakube:~#
root@dakube:~# systemctl status k3s
● k3s.service - Lightweight Kubernetes
     Loaded: loaded (/etc/systemd/system/k3s.service; enabled; preset: enabled)
     Active: active (running) since Mon 2026-01-12 13:24:36 UTC; 6min ago
       Docs: https://k3s.io
    Process: 9121 ExecStartPre=/sbin/modprobe br_netfilter (code=exited, status=0/SUCCESS)
    Process: 9122 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS)
   Main PID: 9123 (k3s-server)
      Tasks: 125
     Memory: 1.2G
        CPU: 3min 26.618s
     CGroup: /system.slice/k3s.service
             ├─ 9123 "/usr/local/bin/k3s server"
             ├─ 9143 "containerd "
             ├─10320 /var/lib/rancher/k3s/data/9b7ee93ee2655b32c83f5e645937519d8a6a866a4d1b1348ea561e7db11514d4/bin/containerd-shim-runc-v2 -namespace k8s.>
             ├─10382 /var/lib/rancher/k3s/data/9b7ee93ee2655b32c83f5e645937519d8a6a866a4d1b1348ea561e7db11514d4/bin/containerd-shim-runc-v2 -namespace k8s.>
             ├─10387 /var/lib/rancher/k3s/data/9b7ee93ee2655b32c83f5e645937519d8a6a866a4d1b1348ea561e7db11514d4/bin/containerd-shim-runc-v2 -namespace k8s.>
             ├─11465 /var/lib/rancher/k3s/data/9b7ee93ee2655b32c83f5e645937519d8a6a866a4d1b1348ea561e7db11514d4/bin/containerd-shim-runc-v2 -namespace k8s.>
             └─11529 /var/lib/rancher/k3s/data/9b7ee93ee2655b32c83f5e645937519d8a6a866a4d1b1348ea561e7db11514d4/bin/containerd-shim-runc-v2 -namespace k8s.>

Jan 12 13:25:40 dakube k3s[9123]: I0112 13:25:40.557033    9123 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="apic>
Jan 12 13:25:40 dakube k3s[9123]: I0112 13:25:40.557197    9123 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" resource="ingr>
Jan 12 13:25:40 dakube k3s[9123]: I0112 13:25:40.558338    9123 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
Jan 12 13:25:41 dakube k3s[9123]: I0112 13:25:41.294124    9123 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
Jan 12 13:25:42 dakube k3s[9123]: I0112 13:25:42.158554    9123 shared_informer.go:356] "Caches are synced" controller="resource quota"
Jan 12 13:25:42 dakube k3s[9123]: I0112 13:25:42.195127    9123 shared_informer.go:356] "Caches are synced" controller="garbage collector"
Jan 12 13:25:44 dakube k3s[9123]: I0112 13:25:44.335423    9123 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/traefi>
Jan 12 13:25:44 dakube k3s[9123]: I0112 13:25:44.386607    9123 event.go:389] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" api>
Jan 12 13:25:45 dakube k3s[9123]: I0112 13:25:45.365119    9123 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" key=>
Jan 12 13:25:45 dakube k3s[9123]: I0112 13:25:45.389370    9123 event.go:389] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" api>
root@dakube:~#
root@dakube:~#
root@dakube:~# export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
root@dakube:~# kubectl get nodes
NAME     STATUS   ROLES                  AGE    VERSION
dakube   Ready    control-plane,master   278d   v1.34.3+k3s1
root@dakube:~#
root@dakube:~#
root@dakube:~# kubectl get pods -A
NAMESPACE     NAME                                      READY   STATUS      RESTARTS   AGE
kube-system   coredns-7f496c8d7d-crgww                  1/1     Running     0          16m
kube-system   helm-install-traefik-crd-6clrn            0/1     Completed   0          16m
kube-system   helm-install-traefik-nsjtj                0/1     Completed   1          16m
kube-system   local-path-provisioner-578895bd58-rtknv   1/1     Running     0          16m
kube-system   metrics-server-7b9c9c4b9c-mp77l           1/1     Running     0          16m
kube-system   svclb-traefik-b4dc2f30-mqgcc              2/2     Running     0          15m
kube-system   traefik-6f5f87584-f552z                   1/1     Running     0          15m
root@dakube:~# crictl ps
CONTAINER           IMAGE               CREATED             STATE               NAME                     ATTEMPT             POD ID              POD                                       NAMESPACE
1fe39f0fc5836       64f5bf38404e1       15 minutes ago      Running             traefik                  0                   294ff254e49c8       traefik-6f5f87584-f552z                   kube-system
64de4a0f4b2b4       a399cf1b9d57c       15 minutes ago      Running             lb-tcp-443               0                   5c8c3703a0cc9       svclb-traefik-b4dc2f30-mqgcc              kube-system
e60fe8565dedb       a399cf1b9d57c       15 minutes ago      Running             lb-tcp-80                0                   5c8c3703a0cc9       svclb-traefik-b4dc2f30-mqgcc              kube-system
2ee42273db290       e08f4d9d2e6ed       15 minutes ago      Running             coredns                  0                   9eaef1d00ac17       coredns-7f496c8d7d-crgww                  kube-system
9465142b08e61       bc6c1e09a843d       15 minutes ago      Running             metrics-server           0                   3d97ce1cf9bbe       metrics-server-7b9c9c4b9c-mp77l           kube-system
f5364477d4909       0594e75cdd5df       16 minutes ago      Running             local-path-provisioner   0                   af0866a43adf9       local-path-provisioner-578895bd58-rtknv   kube-system
root@dakube:~# ls /sys/fs/cgroup/system.slice | grep k3s
k3s.service
mnt-nfs-k3s.mount
root@dakube:~# ls /sys/fs/cgroup/kubepods.slice
cgroup.controllers      cpu.pressure              hugetlb.1GB.rsvd.max       hugetlb.64KB.current       memory.events.local  memory.zswap.current
cgroup.events           cpu.stat                  hugetlb.2MB.current        hugetlb.64KB.events        memory.high          memory.zswap.max
cgroup.freeze           cpu.weight                hugetlb.2MB.events         hugetlb.64KB.events.local  memory.low           misc.current
cgroup.kill             cpu.weight.nice           hugetlb.2MB.events.local   hugetlb.64KB.max           memory.max           misc.events
cgroup.max.depth        cpuset.cpus               hugetlb.2MB.max            hugetlb.64KB.numa_stat     memory.min           misc.max
cgroup.max.descendants  cpuset.cpus.effective     hugetlb.2MB.numa_stat      hugetlb.64KB.rsvd.current  memory.numa_stat     pids.current
cgroup.pressure         cpuset.cpus.partition     hugetlb.2MB.rsvd.current   hugetlb.64KB.rsvd.max      memory.oom.group     pids.events
cgroup.procs            cpuset.mems               hugetlb.2MB.rsvd.max       io.max                     memory.peak          pids.max
cgroup.stat             cpuset.mems.effective     hugetlb.32MB.current       io.pressure                memory.pressure      pids.peak
cgroup.subtree_control  hugetlb.1GB.current       hugetlb.32MB.events        io.stat                    memory.reclaim       rdma.current
cgroup.threads          hugetlb.1GB.events        hugetlb.32MB.events.local  io.weight                  memory.stat          rdma.max
cgroup.type             hugetlb.1GB.events.local  hugetlb.32MB.max           kubepods-besteffort.slice  memory.swap.current
cpu.idle                hugetlb.1GB.max           hugetlb.32MB.numa_stat     kubepods-burstable.slice   memory.swap.events
cpu.max                 hugetlb.1GB.numa_stat     hugetlb.32MB.rsvd.current  memory.current             memory.swap.high
cpu.max.burst           hugetlb.1GB.rsvd.current  hugetlb.32MB.rsvd.max      memory.events              memory.swap.max
root@dakube:~# cat /etc/rancher/k3s/k3s.yaml
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJkekNDQVIyZ0F3SUJBZ0lCQURBS0JnZ3Foa2pPUFFRREFqQWpNU0V3SHdZRFZRUUREQmhyTTNNdGMyVnkKZG1WeUxXTmhRREUzTkRReE5EazJOREl3SGhjTk1qVXdOREE0TWpJd01EUXlXaGNOTXpVd05EQTJNakl3TURReQpXakFqTVNFd0h3WURWUVFEREJock0zTXRjMlZ5ZG1WeUxXTmhRREUzTkRReE5EazJOREl3V1RBVEJnY3Foa2pPClBRSUJCZ2dxaGtqT1BRTUJCd05DQUFUZTFERUR0RVhaMWxqb1UyQ3paazNzdXZJSEVsc3IycWtJdk5WNi85VzEKNkNsc0dpaUFWaG94TXpkcFJCb0pqbExCZFVudFJ3STc0UWxzOVhrUllTaVlvMEl3UURBT0JnTlZIUThCQWY4RQpCQU1DQXFRd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFRmdRVUZzdjRIdUFZc2RlTTdLRUVsaDNOCnlXQ3JiRmt3Q2dZSUtvWkl6ajBFQXdJRFNBQXdSUUloQUxXdW1IWU1ZaW9DcFArSCt1dGtBSTNTMlc2dWdHS1AKZkVBUjd5empZQng5QWlCT01WZkF0a1N5d3VmVldMaitvR00waXk5SzJnYWtEQ2xzekZXOW0yWm50dz09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    server: https://127.0.0.1:6443
  name: default
contexts:
- context:
    cluster: default
    user: default
  name: default
current-context: default
kind: Config
users:
- name: default
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJrakNDQVRlZ0F3SUJBZ0lJVGxPR0pkMnZEVVV3Q2dZSUtvWkl6ajBFQXdJd0l6RWhNQjhHQTFVRUF3d1kKYXpOekxXTnNhV1Z1ZEMxallVQXhOelEwTVRRNU5qUXlNQjRYRFRJMU1EUXdPREl5TURBME1sb1hEVEkzTURFdwpPVEUxTlRJME0xb3dNREVYTUJVR0ExVUVDaE1PYzNsemRHVnRPbTFoYzNSbGNuTXhGVEFUQmdOVkJBTVRESE41CmMzUmxiVHBoWkcxcGJqQlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VIQTBJQUJKOHZRNVJKeWlEcVdma3cKRjZmZ2RMa1lZblRMT21XUFFDWlV5OHh4YVV3MXoranpsWERVNnhxRlpOYU8yMHhJUlFZdzNYQ0xIRHd0a0plbgozUnNKSEFDalNEQkdNQTRHQTFVZER3RUIvd1FFQXdJRm9EQVRCZ05WSFNVRUREQUtCZ2dyQmdFRkJRY0RBakFmCkJnTlZIU01FR0RBV2dCUnJKQzdOSmg3T3psOWR0WjUxNmhmTktmNWpRVEFLQmdncWhrak9QUVFEQWdOSkFEQkcKQWlFQWd5bXhDK21mQUxBU3N4K3pybDBadnFiWHRRNEtpRGdSSVJVUmNKWFZnUDRDSVFEczlwVzNXY0w1cHZSRwo0S21NZW9NKzR2Qk1IbzU0V0g5N1BYUU05Y0pUOEE9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCi0tLS0tQkVHSU4gQ0VSVElGSUNBVEUtLS0tLQpNSUlCZURDQ0FSMmdBd0lCQWdJQkFEQUtCZ2dxaGtqT1BRUURBakFqTVNFd0h3WURWUVFEREJock0zTXRZMnhwClpXNTBMV05oUURFM05EUXhORGsyTkRJd0hoY05NalV3TkRBNE1qSXdNRFF5V2hjTk16VXdOREEyTWpJd01EUXkKV2pBak1TRXdId1lEVlFRRERCaHJNM010WTJ4cFpXNTBMV05oUURFM05EUXhORGsyTkRJd1dUQVRCZ2NxaGtqTwpQUUlCQmdncWhrak9QUU1CQndOQ0FBUlNqNUxGcDZLdFM1N1dBTEtuR1ZHR0F1ckJQNXlKM21NREd0OGsyREhtCmxLbWozZDdjUGpIV2hFQ09qRW1zVWJIMHdyaEsxWFl3TVlTZVlxZDROdGVPbzBJd1FEQU9CZ05WSFE4QkFmOEUKQkFNQ0FxUXdEd1lEVlIwVEFRSC9CQVV3QXdFQi96QWRCZ05WSFE0RUZnUVVheVF1elNZZXpzNWZYYldlZGVvWAp6U24rWTBFd0NnWUlLb1pJemowRUF3SURTUUF3UmdJaEFNbCtFbE9aY0tsdUFhcWJwWmhYWjVybnJhMlozb1ltClBWTGdkQjJ6Mk9GU0FpRUFtY0hVczByTEFaV3VEcm5JYkVJb3ZYa0ZIY3BBc3BmTnE3MGJ3RUhRYXlvPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    client-key-data: LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSUIrVTRobFJyMmdaQjgvazROTVMyWGZEOTRMK0phNENVZTZBRHRjeFZDVVFvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFbnk5RGxFbktJT3BaK1RBWHArQjB1UmhpZE1zNlpZOUFKbFRMekhGcFREWFA2UE9WY05UcgpHb1ZrMW83YlRFaEZCakRkY0lzY1BDMlFsNmZkR3drY0FBPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=


TP 4.2

~$ scp -o ProxyCommand="ssh rouls@dakube sudo cat /etc/rancher/k3s/k3s.yaml" \
    rouls@dakube:/etc/hosts ~/.kube/dakube.yaml
Enter passphrase for key '/home/rouls/.ssh/id_ed25519_rpi_devops':
~$vi ~/.kube/dakube.yaml

~$ kubectl get nodes
NAME     STATUS   ROLES                  AGE    VERSION
dakube   Ready    control-plane,master   278d   v1.34.3+k3s1

~$ kubectl config get-contexts
CURRENT   NAME      CLUSTER   AUTHINFO   NAMESPACE
*         default   default   default
~$ kubectl config use-context default
Switched to context "default".
~$ echo 'alias k=kubectl' >> ~/.bashrc
source ~/.bashrc
~$ k config get-contexts
CURRENT   NAME      CLUSTER   AUTHINFO   NAMESPACE
*         default   default   default


TP 5.1

rouls@wsl:~$ echo 'alias k=kubectl' >> ~/.bashrc
source ~/.bashrc
rouls@wsl:~$
rouls@wsl:~$
rouls@wsl:~$ k config get-context
error: unknown command "get-context"
See 'kubectl config -h' for help and examples
rouls@wsl:~$ k config get-contexts
CURRENT   NAME      CLUSTER   AUTHINFO   NAMESPACE
*         default   default   default
rouls@wsl:~$
rouls@wsl:~$
rouls@wsl:~$ kubectl get nodes
NAME     STATUS   ROLES                  AGE    VERSION
dakube   Ready    control-plane,master   278d   v1.34.3+k3s1
rouls@wsl:~$ k config get-contexts
CURRENT   NAME      CLUSTER   AUTHINFO   NAMESPACE
*         default   default   default
rouls@wsl:~$ k config current-context
default
rouls@wsl:~$ k get nodes -o wide
NAME     STATUS   ROLES                  AGE    VERSION        INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION   CONTAINER-RUNTIME
dakube   Ready    control-plane,master   278d   v1.34.3+k3s1   192.168.1.5   <none>        Debian GNU/Linux 12 (bookworm)   6.1.0-41-arm64   containerd://2.1.5-k3s1
rouls@wsl:~$ kubectl run ktest \
  --image=nginx \
  --restart=Never
pod/ktest created
rouls@wsl:~$ k get pods
NAME    READY   STATUS              RESTARTS   AGE
ktest   0/1     ContainerCreating   0          10s
rouls@wsl:~$
rouls@wsl:~$
rouls@wsl:~$ k get pods
NAME    READY   STATUS              RESTARTS   AGE
ktest   0/1     ContainerCreating   0          14s
rouls@wsl:~$
rouls@wsl:~$
rouls@wsl:~$ k get pods
NAME    READY   STATUS              RESTARTS   AGE
ktest   0/1     ContainerCreating   0          20s
rouls@wsl:~$
rouls@wsl:~$
rouls@wsl:~$ k get pods
NAME    READY   STATUS              RESTARTS   AGE
ktest   0/1     ContainerCreating   0          23s
rouls@wsl:~$
rouls@wsl:~$
rouls@wsl:~$ k get pods
NAME    READY   STATUS    RESTARTS   AGE
ktest   1/1     Running   0          25s
rouls@wsl:~$
rouls@wsl:~$
rouls@wsl:~$ k get pods
NAME    READY   STATUS    RESTARTS   AGE
ktest   1/1     Running   0          27s
rouls@wsl:~$
rouls@wsl:~$
rouls@wsl:~$ k describe pod ktest
Name:             ktest
Namespace:        default
Priority:         0
Service Account:  default
Node:             dakube/192.168.1.5
Start Time:       Mon, 12 Jan 2026 18:08:22 +0100
Labels:           run=ktest
Annotations:      <none>
Status:           Running
IP:               10.42.1.152
IPs:
  IP:  10.42.1.152
Containers:
  ktest:
    Container ID:   containerd://bdb96a247fe1fb07a02b651b8c237c3b09fc7bc8c78e5b0008fae46cf7812326
    Image:          nginx
    Image ID:       docker.io/library/nginx@sha256:7272239bd21472f311aa3e86a85fdca0f1ad648995f983ab6e5e7dea665cd233
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Mon, 12 Jan 2026 18:08:46 +0100
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dlgvh (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       True
  ContainersReady             True
  PodScheduled                True
Volumes:
  kube-api-access-dlgvh:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  39s   default-scheduler  Successfully assigned default/ktest to dakube
  Normal  Pulling    38s   kubelet            Pulling image "nginx"
  Normal  Pulled     15s   kubelet            Successfully pulled image "nginx" in 22.685s (22.685s including waiting). Image size: 61205168 bytes.
  Normal  Created    15s   kubelet            Created container: ktest
  Normal  Started    15s   kubelet            Started container ktest
rouls@wsl:~$ kubectl exec -it ktest -- sh
# hostnam
sh: 1: hostnam: not found
# hostname
ktest
# ip addr
sh: 3: ip: not found
# ip
sh: 4: ip: not found
#
#
# ifconfig -a
sh: 7: ifconfig: not found
# exit
command terminated with exit code 127
rouls@wsl:~$
rouls@wsl:~$
rouls@wsl:~$ k describe pod ktest
Name:             ktest
Namespace:        default
Priority:         0
Service Account:  default
Node:             dakube/192.168.1.5
Start Time:       Mon, 12 Jan 2026 18:08:22 +0100
Labels:           run=ktest
Annotations:      <none>
Status:           Running
IP:               10.42.1.152
IPs:
  IP:  10.42.1.152
Containers:
  ktest:
    Container ID:   containerd://bdb96a247fe1fb07a02b651b8c237c3b09fc7bc8c78e5b0008fae46cf7812326
    Image:          nginx
    Image ID:       docker.io/library/nginx@sha256:7272239bd21472f311aa3e86a85fdca0f1ad648995f983ab6e5e7dea665cd233
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Mon, 12 Jan 2026 18:08:46 +0100
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dlgvh (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       True
  ContainersReady             True
  PodScheduled                True
Volumes:
  kube-api-access-dlgvh:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  3m49s  default-scheduler  Successfully assigned default/ktest to dakube
  Normal  Pulling    3m48s  kubelet            Pulling image "nginx"
  Normal  Pulled     3m25s  kubelet            Successfully pulled image "nginx" in 22.685s (22.685s including waiting). Image size: 61205168 bytes.
  Normal  Created    3m25s  kubelet            Created container: ktest
  Normal  Started    3m25s  kubelet            Started container ktest
rouls@wsl:~$ k get pods
NAME    READY   STATUS    RESTARTS   AGE
ktest   1/1     Running   0          3m57s
rouls@wsl:~$
rouls@wsl:~$
rouls@wsl:~$ k logs ktest
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
/docker-entrypoint.sh: Sourcing /docker-entrypoint.d/15-local-resolvers.envsh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
/docker-entrypoint.sh: Configuration complete; ready for start up
2026/01/12 17:08:46 [notice] 1#1: using the "epoll" event method
2026/01/12 17:08:46 [notice] 1#1: nginx/1.29.4
2026/01/12 17:08:46 [notice] 1#1: built by gcc 14.2.0 (Debian 14.2.0-19)
2026/01/12 17:08:46 [notice] 1#1: OS: Linux 6.1.0-41-arm64
2026/01/12 17:08:46 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576
2026/01/12 17:08:46 [notice] 1#1: start worker processes
2026/01/12 17:08:46 [notice] 1#1: start worker process 29
2026/01/12 17:08:46 [notice] 1#1: start worker process 30
2026/01/12 17:08:46 [notice] 1#1: start worker process 31
2026/01/12 17:08:46 [notice] 1#1: start worker process 32
rouls@wsl:~$
rouls@wsl:~$
rouls@wsl:~$ k delete pod ktest
pod "ktest" deleted from default namespace
rouls@wsl:~$
rouls@wsl:~$ k get pods
No resources found in default namespace.


TP 5.2

$ ip addr | grep -E "flannel|cni"
5: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default
    inet 10.42.1.0/32 scope global flannel.1
6: cni0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default qlen 1000
    inet 10.42.1.1/24 brd 10.42.1.255 scope global cni0
17: vethfc572508@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP group default
    link/ether aa:57:a7:de:57:a2 brd ff:ff:ff:ff:ff:ff link-netns cni-c2357678-b01c-529e-6276-c325abbee97b
19: vethf7e0e918@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP group default
    link/ether 4e:f2:ad:d5:ab:b1 brd ff:ff:ff:ff:ff:ff link-netns cni-2c818594-caf4-dc6a-c993-199fe393066a
20: vethd9f0ac78@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP group default
    link/ether 9a:cd:9c:50:ab:51 brd ff:ff:ff:ff:ff:ff link-netns cni-30e62309-1b9e-6672-e0c5-1da79e626912
21: veth42b523c1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP group default
    link/ether b6:e8:2b:ae:8d:73 brd ff:ff:ff:ff:ff:ff link-netns cni-538e8d47-4637-c1e4-65fa-7bd7cfb5424f
22: veth83eb26da@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP group default
    link/ether e2:a5:ab:04:23:59 brd ff:ff:ff:ff:ff:ff link-netns cni-2bab7dea-c40c-4a42-e30f-e7eaa58d0d1a
$ sudo cat /var/lib/rancher/k3s/agent/etc/flannel/net-conf.json
[sudo] password for rouls:
{
        "Network": "10.42.0.0/16",
        "EnableIPv6": false,
        "EnableIPv4": true,
        "IPv6Network": "::/0",
        "Backend": {
        "Type": "vxlan"
}
}
$ ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host noprefixroute
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether dc:a6:32:29:12:36 brd ff:ff:ff:ff:ff:ff
    altname end0
    inet 192.168.1.5/24 brd 192.168.1.255 scope global dynamic eth0
       valid_lft 40682sec preferred_lft 40682sec
    inet6 2a01:e0a:f7c:44e0:dea6:32ff:fe29:1236/64 scope global dynamic mngtmpaddr
       valid_lft 86105sec preferred_lft 86105sec
    inet6 fe80::dea6:32ff:fe29:1236/64 scope link
       valid_lft forever preferred_lft forever
3: wlan0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000
    link/ether dc:a6:32:29:12:37 brd ff:ff:ff:ff:ff:ff
4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default
    link/ether c6:54:94:f4:3f:a4 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
    inet6 fe80::c454:94ff:fef4:3fa4/64 scope link
       valid_lft forever preferred_lft forever
5: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default
    link/ether 2e:08:f1:66:11:81 brd ff:ff:ff:ff:ff:ff
    inet 10.42.1.0/32 scope global flannel.1
       valid_lft forever preferred_lft forever
    inet6 fe80::2c08:f1ff:fe66:1181/64 scope link
       valid_lft forever preferred_lft forever
6: cni0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default qlen 1000
    link/ether 9a:ff:98:81:94:83 brd ff:ff:ff:ff:ff:ff
    inet 10.42.1.1/24 brd 10.42.1.255 scope global cni0
       valid_lft forever preferred_lft forever
    inet6 fe80::98ff:98ff:fe81:9483/64 scope link
       valid_lft forever preferred_lft forever
17: vethfc572508@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP group default
    link/ether aa:57:a7:de:57:a2 brd ff:ff:ff:ff:ff:ff link-netns cni-c2357678-b01c-529e-6276-c325abbee97b
    inet6 fe80::a857:a7ff:fede:57a2/64 scope link
       valid_lft forever preferred_lft forever
19: vethf7e0e918@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP group default
    link/ether 4e:f2:ad:d5:ab:b1 brd ff:ff:ff:ff:ff:ff link-netns cni-2c818594-caf4-dc6a-c993-199fe393066a
    inet6 fe80::4cf2:adff:fed5:abb1/64 scope link
       valid_lft forever preferred_lft forever
20: vethd9f0ac78@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP group default
    link/ether 9a:cd:9c:50:ab:51 brd ff:ff:ff:ff:ff:ff link-netns cni-30e62309-1b9e-6672-e0c5-1da79e626912
    inet6 fe80::98cd:9cff:fe50:ab51/64 scope link
       valid_lft forever preferred_lft forever
21: veth42b523c1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP group default
    link/ether b6:e8:2b:ae:8d:73 brd ff:ff:ff:ff:ff:ff link-netns cni-538e8d47-4637-c1e4-65fa-7bd7cfb5424f
    inet6 fe80::b4e8:2bff:feae:8d73/64 scope link
       valid_lft forever preferred_lft forever
22: veth83eb26da@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP group default
    link/ether e2:a5:ab:04:23:59 brd ff:ff:ff:ff:ff:ff link-netns cni-2bab7dea-c40c-4a42-e30f-e7eaa58d0d1a
    inet6 fe80::e0a5:abff:fe04:2359/64 scope link
       valid_lft forever preferred_lft forever
$ ip addr show cni0
6: cni0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default qlen 1000
    link/ether 9a:ff:98:81:94:83 brd ff:ff:ff:ff:ff:ff
    inet 10.42.1.1/24 brd 10.42.1.255 scope global cni0
       valid_lft forever preferred_lft forever
    inet6 fe80::98ff:98ff:fe81:9483/64 scope link
       valid_lft forever preferred_lft forever
$
$
$ ip addr show flannel.1
5: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default
    link/ether 2e:08:f1:66:11:81 brd ff:ff:ff:ff:ff:ff
    inet 10.42.1.0/32 scope global flannel.1
       valid_lft forever preferred_lft forever
    inet6 fe80::2c08:f1ff:fe66:1181/64 scope link
       valid_lft forever preferred_lft forever
$
$
$ ip link show flannel.1
5: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default
    link/ether 2e:08:f1:66:11:81 brd ff:ff:ff:ff:ff:ff
$ ip route
default via 192.168.1.254 dev eth0
10.42.1.0/24 dev cni0 proto kernel scope link src 10.42.1.1
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
192.168.1.0/24 dev eth0 proto kernel scope link src 192.168.1.5
$ ip link | grep veth
17: vethfc572508@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP mode DEFAULT group default
19: vethf7e0e918@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP mode DEFAULT group default
20: vethd9f0ac78@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP mode DEFAULT group default
21: veth42b523c1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP mode DEFAULT group default
22: veth83eb26da@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP mode DEFAULT group default
$ kubectl run pod-a --image=busybox --restart=Never -- sleep 3600
WARN[0001] Unable to read /etc/rancher/k3s/k3s.yaml, please start server with --write-kubeconfig-mode or --write-kubeconfig-group to modify kube config permissions
error: error loading config file "/etc/rancher/k3s/k3s.yaml": open /etc/rancher/k3s/k3s.yaml: permission denied
$ sudo -i
[sudo] password for rouls:
root@dakube:~# kubectl run pod-a --image=busybox --restart=Never -- sleep 3600
pod/pod-a created
root@dakube:~# crictl ps
CONTAINER           IMAGE               CREATED             STATE               NAME                     ATTEMPT             POD ID              POD                                       NAMESPACE
78004d5817093       eade5be814e81       4 seconds ago       Running             pod-a                    0                   3332bbbbfb3f6       pod-a                                     default
1fe39f0fc5836       64f5bf38404e1       32 hours ago        Running             traefik                  0                   294ff254e49c8       traefik-6f5f87584-f552z                   kube-system
64de4a0f4b2b4       a399cf1b9d57c       32 hours ago        Running             lb-tcp-443               0                   5c8c3703a0cc9       svclb-traefik-b4dc2f30-mqgcc              kube-system
e60fe8565dedb       a399cf1b9d57c       32 hours ago        Running             lb-tcp-80                0                   5c8c3703a0cc9       svclb-traefik-b4dc2f30-mqgcc              kube-system
2ee42273db290       e08f4d9d2e6ed       32 hours ago        Running             coredns                  0                   9eaef1d00ac17       coredns-7f496c8d7d-crgww                  kube-system
9465142b08e61       bc6c1e09a843d       32 hours ago        Running             metrics-server           0                   3d97ce1cf9bbe       metrics-server-7b9c9c4b9c-mp77l           kube-system
f5364477d4909       0594e75cdd5df       32 hours ago        Running             local-path-provisioner   0                   af0866a43adf9       local-path-provisioner-578895bd58-rtknv   kube-system
root@dakube:~# man crictl
-bash: man: command not found
root@dakube:~# crictl --help
NAME:
   crictl - client for CRI

USAGE:
   crictl [global options] command [command options]

VERSION:
   v1.34.0-k3s2

COMMANDS:
   attach                 Attach to a running container
   checkpoint             Checkpoint one or more running containers
   completion             Output shell completion code
   config                 Get, set and list crictl configuration options
   create                 Create a new container
   events, event          Stream the events of containers
   exec                   Run a command in a running container
   imagefsinfo            Return image filesystem info
   images, image, img     List images
   info                   Display information of the container runtime
   inspect                Display the status of one or more containers
   inspecti               Return the status of one or more images
   inspectp               Display the status of one or more pods
   logs                   Fetch the logs of a container
   metricdescs            List metric descriptors. Returns information about the metrics available through the CRI.
   metricsp               List pod metrics. Metrics are unstructured key/value pairs gathered by CRI meant to replace cAdvisor's /metrics/cadvisor endpoint.
   pods                   List pods
   port-forward           Forward local port to a pod
   ps                     List containers
   pull                   Pull an image from a registry
   rm                     Remove one or more containers
   rmi                    Remove one or more images
   rmp                    Remove one or more pods
   run                    Run a new container inside a sandbox
   runp                   Run a new pod
   runtime-config         Retrieve the container runtime configuration
   start                  Start one or more created containers
   stats                  List container(s) resource usage statistics
   statsp                 List pod statistics. Stats represent a structured API that will fulfill the Kubelet's /stats/summary endpoint.
   stop                   Stop one or more running containers
   stopp                  Stop one or more running pods
   update                 Update one or more running containers
   update-runtime-config  Update the runtime configuration
   version                Display runtime version information
   help, h                Shows a list of commands or help for one command

GLOBAL OPTIONS:
   --config value, -c value                   Location of the client config file. If not specified and the default does not exist, the program's directory is searched as well (default: "/etc/crictl.yaml") [$CRI_CONFIG_FILE]
   --debug, -D                                Enable debug mode (default: false)
   --enable-tracing                           Enable OpenTelemetry tracing. (default: false)
   --image-endpoint value, -i value           Endpoint of CRI image manager service (default: uses 'runtime-endpoint' setting) [$IMAGE_SERVICE_ENDPOINT]
   --profile-cpu value                        Write a pprof CPU profile to the provided path.
   --profile-mem value                        Write a pprof memory profile to the provided path.
   --runtime-endpoint value, -r value         Endpoint of CRI container runtime service (default: uses in order the first successful one of [unix:///run/k3s/containerd/containerd.sock unix:///var/run/dockershim.sock unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]). Default is now deprecated and the endpoint should be set instead. [$CONTAINER_RUNTIME_ENDPOINT]
   --timeout value, -t value                  Timeout of connecting to the server in seconds (e.g. 2s, 20s.). 0 or less is set to default (default: 2s)
   --tracing-endpoint value                   Address to which the gRPC tracing collector will send spans to. (default: "127.0.0.1:4317")
   --tracing-sampling-rate-per-million value  Number of samples to collect per million OpenTelemetry spans. Set to 1000000 or -1 to always sample. (default: -1)
   --help, -h                                 Show help (default: false)
   --version, -v                              Print the version (default: false)
root@dakube:~# crictl inspect 3332bbbbfb3f6 |grep -i netns
E0113 21:28:20.802083   82709 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = NotFound desc = an error occurred when try to find container \"3332bbbbfb3f6\": not found" containerID="3332bbbbfb3f6"
FATA[0000] get the status of containers: get container status: rpc error: code = NotFound desc = an error occurred when try to find container "3332bbbbfb3f6": not found
root@dakube:~# crictl inspect 78004d5817093 |grep -i netns
root@dakube:~#
root@dakube:~#
root@dakube:~# crictl inspect 78004d5817093
{
  "info": {
    "config": {
      "annotations": {
        "io.kubernetes.container.hash": "76c3a631",
        "io.kubernetes.container.restartCount": "0",
        "io.kubernetes.container.terminationMessagePath": "/dev/termination-log",
        "io.kubernetes.container.terminationMessagePolicy": "File",
        "io.kubernetes.pod.terminationGracePeriod": "30"
      },
      "args": [
        "sleep",
        "3600"
      ],
      "envs": [
        {
          "key": "KUBERNETES_SERVICE_HOST",
          "value": "10.43.0.1"
        },
        {
          "key": "KUBERNETES_PORT_443_TCP_PROTO",
          "value": "tcp"
        },
        {
          "key": "MYAPP_SERVICE_PORT_80_TCP",
          "value": "tcp://10.43.62.254:80"
        },
        {
          "key": "KUBERNETES_PORT",
          "value": "tcp://10.43.0.1:443"
        },
        {
          "key": "MYAPP_SERVICE_PORT_80_TCP_PORT",
          "value": "80"
        },
        {
          "key": "MYAPP_SERVICE_PORT_80_TCP_ADDR",
          "value": "10.43.62.254"
        },
        {
          "key": "KUBERNETES_SERVICE_PORT_HTTPS",
          "value": "443"
        },
        {
          "key": "KUBERNETES_PORT_443_TCP",
          "value": "tcp://10.43.0.1:443"
        },
        {
          "key": "MYAPP_SERVICE_SERVICE_PORT",
          "value": "80"
        },
        {
          "key": "MYAPP_SERVICE_PORT",
          "value": "tcp://10.43.62.254:80"
        },
        {
          "key": "KUBERNETES_SERVICE_PORT",
          "value": "443"
        },
        {
          "key": "KUBERNETES_PORT_443_TCP_PORT",
          "value": "443"
        },
        {
          "key": "KUBERNETES_PORT_443_TCP_ADDR",
          "value": "10.43.0.1"
        },
        {
          "key": "MYAPP_SERVICE_SERVICE_HOST",
          "value": "10.43.62.254"
        },
        {
          "key": "MYAPP_SERVICE_PORT_80_TCP_PROTO",
          "value": "tcp"
        }
      ],
      "image": {
        "image": "sha256:eade5be814e817df411f138aa7711c3f81595185eb54b3257fd19f6c4966b285",
        "user_specified_image": "busybox"
      },
      "labels": {
        "io.kubernetes.container.name": "pod-a",
        "io.kubernetes.pod.name": "pod-a",
        "io.kubernetes.pod.namespace": "default",
        "io.kubernetes.pod.uid": "c28a4ef5-871f-4f83-87c7-8c6f9a2f4658"
      },
      "linux": {
        "resources": {
          "cpu_period": 100000,
          "cpu_shares": 2,
          "hugepage_limits": [
            {
              "page_size": "2MB"
            },
            {
              "page_size": "32MB"
            },
            {
              "page_size": "64KB"
            },
            {
              "page_size": "1GB"
            }
          ],
          "oom_score_adj": 1000,
          "unified": {
            "memory.oom.group": "1",
            "memory.swap.max": "0"
          }
        },
        "security_context": {
          "masked_paths": [
            "/proc/asound",
            "/proc/acpi",
            "/proc/interrupts",
            "/proc/kcore",
            "/proc/keys",
            "/proc/latency_stats",
            "/proc/timer_list",
            "/proc/timer_stats",
            "/proc/sched_debug",
            "/proc/scsi",
            "/sys/firmware",
            "/sys/devices/virtual/powercap"
          ],
          "namespace_options": {
            "pid": 1,
            "userns_options": {
              "mode": 2
            }
          },
          "readonly_paths": [
            "/proc/bus",
            "/proc/fs",
            "/proc/irq",
            "/proc/sys",
            "/proc/sysrq-trigger"
          ],
          "run_as_user": {},
          "seccomp": {
            "profile_type": 1
          }
        }
      },
      "log_path": "pod-a/0.log",
      "metadata": {
        "name": "pod-a"
      },
      "mounts": [
        {
          "container_path": "/var/run/secrets/kubernetes.io/serviceaccount",
          "host_path": "/var/lib/kubelet/pods/c28a4ef5-871f-4f83-87c7-8c6f9a2f4658/volumes/kubernetes.io~projected/kube-api-access-ts47l",
          "readonly": true
        },
        {
          "container_path": "/etc/hosts",
          "host_path": "/var/lib/kubelet/pods/c28a4ef5-871f-4f83-87c7-8c6f9a2f4658/etc-hosts"
        },
        {
          "container_path": "/dev/termination-log",
          "host_path": "/var/lib/kubelet/pods/c28a4ef5-871f-4f83-87c7-8c6f9a2f4658/containers/pod-a/b8e4cdfb"
        }
      ]
    },
    "pid": 82540,
    "removing": false,
    "runtimeOptions": {
      "systemd_cgroup": true
    },
    "runtimeSpec": {
      "annotations": {
        "io.kubernetes.cri.container-name": "pod-a",
        "io.kubernetes.cri.container-type": "container",
        "io.kubernetes.cri.image-name": "busybox",
        "io.kubernetes.cri.sandbox-id": "3332bbbbfb3f6c8e339c28ec9b0582323ec7e91288bebec7af0487b7f2772190",
        "io.kubernetes.cri.sandbox-name": "pod-a",
        "io.kubernetes.cri.sandbox-namespace": "default",
        "io.kubernetes.cri.sandbox-uid": "c28a4ef5-871f-4f83-87c7-8c6f9a2f4658"
      },
      "linux": {
        "cgroupsPath": "kubepods-besteffort-podc28a4ef5_871f_4f83_87c7_8c6f9a2f4658.slice:cri-containerd:78004d58170939265d76db487717874ea44e079e4379bcc39f8be9e1a0d63a2b",
        "maskedPaths": [
          "/proc/asound",
          "/proc/acpi",
          "/proc/interrupts",
          "/proc/kcore",
          "/proc/keys",
          "/proc/latency_stats",
          "/proc/timer_list",
          "/proc/timer_stats",
          "/proc/sched_debug",
          "/proc/scsi",
          "/sys/firmware",
          "/sys/devices/virtual/powercap"
        ],
        "namespaces": [
          {
            "type": "pid"
          },
          {
            "path": "/proc/82511/ns/ipc",
            "type": "ipc"
          },
          {
            "path": "/proc/82511/ns/uts",
            "type": "uts"
          },
          {
            "type": "mount"
          },
          {
            "path": "/proc/82511/ns/net",
            "type": "network"
          },
          {
            "type": "cgroup"
          }
        ],
        "readonlyPaths": [
          "/proc/bus",
          "/proc/fs",
          "/proc/irq",
          "/proc/sys",
          "/proc/sysrq-trigger"
        ],
        "resources": {
          "cpu": {
            "period": 100000,
            "shares": 2
          },
          "devices": [
            {
              "access": "rwm",
              "allow": false
            }
          ],
          "memory": {},
          "unified": {
            "memory.oom.group": "1",
            "memory.swap.max": "0"
          }
        }
      },
      "mounts": [
        {
          "destination": "/proc",
          "options": [
            "nosuid",
            "noexec",
            "nodev"
          ],
          "source": "proc",
          "type": "proc"
        },
        {
          "destination": "/dev",
          "options": [
            "nosuid",
            "strictatime",
            "mode=755",
            "size=65536k"
          ],
          "source": "tmpfs",
          "type": "tmpfs"
        },
        {
          "destination": "/dev/pts",
          "options": [
            "nosuid",
            "noexec",
            "newinstance",
            "ptmxmode=0666",
            "mode=0620",
            "gid=5"
          ],
          "source": "devpts",
          "type": "devpts"
        },
        {
          "destination": "/dev/mqueue",
          "options": [
            "nosuid",
            "noexec",
            "nodev"
          ],
          "source": "mqueue",
          "type": "mqueue"
        },
        {
          "destination": "/sys",
          "options": [
            "nosuid",
            "noexec",
            "nodev",
            "ro"
          ],
          "source": "sysfs",
          "type": "sysfs"
        },
        {
          "destination": "/sys/fs/cgroup",
          "options": [
            "nosuid",
            "noexec",
            "nodev",
            "relatime",
            "ro"
          ],
          "source": "cgroup",
          "type": "cgroup"
        },
        {
          "destination": "/etc/hosts",
          "options": [
            "rbind",
            "rprivate",
            "rw"
          ],
          "source": "/var/lib/kubelet/pods/c28a4ef5-871f-4f83-87c7-8c6f9a2f4658/etc-hosts",
          "type": "bind"
        },
        {
          "destination": "/dev/termination-log",
          "options": [
            "rbind",
            "rprivate",
            "rw"
          ],
          "source": "/var/lib/kubelet/pods/c28a4ef5-871f-4f83-87c7-8c6f9a2f4658/containers/pod-a/b8e4cdfb",
          "type": "bind"
        },
        {
          "destination": "/etc/hostname",
          "options": [
            "rbind",
            "rprivate",
            "rw"
          ],
          "source": "/var/lib/rancher/k3s/agent/containerd/io.containerd.grpc.v1.cri/sandboxes/3332bbbbfb3f6c8e339c28ec9b0582323ec7e91288bebec7af0487b7f2772190/hostname",
          "type": "bind"
        },
        {
          "destination": "/etc/resolv.conf",
          "options": [
            "rbind",
            "rprivate",
            "rw"
          ],
          "source": "/var/lib/rancher/k3s/agent/containerd/io.containerd.grpc.v1.cri/sandboxes/3332bbbbfb3f6c8e339c28ec9b0582323ec7e91288bebec7af0487b7f2772190/resolv.conf",
          "type": "bind"
        },
        {
          "destination": "/dev/shm",
          "options": [
            "rbind",
            "rprivate",
            "rw"
          ],
          "source": "/run/k3s/containerd/io.containerd.grpc.v1.cri/sandboxes/3332bbbbfb3f6c8e339c28ec9b0582323ec7e91288bebec7af0487b7f2772190/shm",
          "type": "bind"
        },
        {
          "destination": "/var/run/secrets/kubernetes.io/serviceaccount",
          "options": [
            "rbind",
            "rprivate",
            "ro"
          ],
          "source": "/var/lib/kubelet/pods/c28a4ef5-871f-4f83-87c7-8c6f9a2f4658/volumes/kubernetes.io~projected/kube-api-access-ts47l",
          "type": "bind"
        }
      ],
      "ociVersion": "1.2.1",
      "process": {
        "apparmorProfile": "cri-containerd.apparmor.d",
        "args": [
          "sleep",
          "3600"
        ],
        "capabilities": {
          "bounding": [
            "CAP_CHOWN",
            "CAP_DAC_OVERRIDE",
            "CAP_FSETID",
            "CAP_FOWNER",
            "CAP_MKNOD",
            "CAP_NET_RAW",
            "CAP_SETGID",
            "CAP_SETUID",
            "CAP_SETFCAP",
            "CAP_SETPCAP",
            "CAP_NET_BIND_SERVICE",
            "CAP_SYS_CHROOT",
            "CAP_KILL",
            "CAP_AUDIT_WRITE"
          ],
          "effective": [
            "CAP_CHOWN",
            "CAP_DAC_OVERRIDE",
            "CAP_FSETID",
            "CAP_FOWNER",
            "CAP_MKNOD",
            "CAP_NET_RAW",
            "CAP_SETGID",
            "CAP_SETUID",
            "CAP_SETFCAP",
            "CAP_SETPCAP",
            "CAP_NET_BIND_SERVICE",
            "CAP_SYS_CHROOT",
            "CAP_KILL",
            "CAP_AUDIT_WRITE"
          ],
          "permitted": [
            "CAP_CHOWN",
            "CAP_DAC_OVERRIDE",
            "CAP_FSETID",
            "CAP_FOWNER",
            "CAP_MKNOD",
            "CAP_NET_RAW",
            "CAP_SETGID",
            "CAP_SETUID",
            "CAP_SETFCAP",
            "CAP_SETPCAP",
            "CAP_NET_BIND_SERVICE",
            "CAP_SYS_CHROOT",
            "CAP_KILL",
            "CAP_AUDIT_WRITE"
          ]
        },
        "cwd": "/",
        "env": [
          "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin",
          "HOSTNAME=pod-a",
          "KUBERNETES_SERVICE_HOST=10.43.0.1",
          "KUBERNETES_PORT_443_TCP_PROTO=tcp",
          "MYAPP_SERVICE_PORT_80_TCP=tcp://10.43.62.254:80",
          "KUBERNETES_PORT=tcp://10.43.0.1:443",
          "MYAPP_SERVICE_PORT_80_TCP_PORT=80",
          "MYAPP_SERVICE_PORT_80_TCP_ADDR=10.43.62.254",
          "KUBERNETES_SERVICE_PORT_HTTPS=443",
          "KUBERNETES_PORT_443_TCP=tcp://10.43.0.1:443",
          "MYAPP_SERVICE_SERVICE_PORT=80",
          "MYAPP_SERVICE_PORT=tcp://10.43.62.254:80",
          "KUBERNETES_SERVICE_PORT=443",
          "KUBERNETES_PORT_443_TCP_PORT=443",
          "KUBERNETES_PORT_443_TCP_ADDR=10.43.0.1",
          "MYAPP_SERVICE_SERVICE_HOST=10.43.62.254",
          "MYAPP_SERVICE_PORT_80_TCP_PROTO=tcp"
        ],
        "oomScoreAdj": 1000,
        "user": {
          "additionalGids": [
            0,
            10
          ],
          "gid": 0,
          "uid": 0
        }
      },
      "root": {
        "path": "rootfs"
      }
    },
    "runtimeType": "io.containerd.runc.v2",
    "sandboxID": "3332bbbbfb3f6c8e339c28ec9b0582323ec7e91288bebec7af0487b7f2772190",
    "snapshotKey": "78004d58170939265d76db487717874ea44e079e4379bcc39f8be9e1a0d63a2b",
    "snapshotter": "overlayfs"
  },
  "status": {
    "annotations": {
      "io.kubernetes.container.hash": "76c3a631",
      "io.kubernetes.container.restartCount": "0",
      "io.kubernetes.container.terminationMessagePath": "/dev/termination-log",
      "io.kubernetes.container.terminationMessagePolicy": "File",
      "io.kubernetes.pod.terminationGracePeriod": "30"
    },
    "createdAt": "2026-01-13T21:25:52.287223177Z",
    "exitCode": 0,
    "finishedAt": "0001-01-01T00:00:00Z",
    "id": "78004d58170939265d76db487717874ea44e079e4379bcc39f8be9e1a0d63a2b",
    "image": {
      "annotations": {},
      "image": "docker.io/library/busybox:latest",
      "runtimeHandler": "",
      "userSpecifiedImage": ""
    },
    "imageId": "",
    "imageRef": "docker.io/library/busybox@sha256:2383baad1860bbe9d8a7a843775048fd07d8afe292b94bd876df64a69aae7cb1",
    "labels": {
      "io.kubernetes.container.name": "pod-a",
      "io.kubernetes.pod.name": "pod-a",
      "io.kubernetes.pod.namespace": "default",
      "io.kubernetes.pod.uid": "c28a4ef5-871f-4f83-87c7-8c6f9a2f4658"
    },
    "logPath": "/var/log/pods/default_pod-a_c28a4ef5-871f-4f83-87c7-8c6f9a2f4658/pod-a/0.log",
    "message": "",
    "metadata": {
      "attempt": 0,
      "name": "pod-a"
    },
    "mounts": [
      {
        "containerPath": "/var/run/secrets/kubernetes.io/serviceaccount",
        "gidMappings": [],
        "hostPath": "/var/lib/kubelet/pods/c28a4ef5-871f-4f83-87c7-8c6f9a2f4658/volumes/kubernetes.io~projected/kube-api-access-ts47l",
        "imageSubPath": "",
        "propagation": "PROPAGATION_PRIVATE",
        "readonly": true,
        "recursiveReadOnly": false,
        "selinuxRelabel": false,
        "uidMappings": []
      },
      {
        "containerPath": "/etc/hosts",
        "gidMappings": [],
        "hostPath": "/var/lib/kubelet/pods/c28a4ef5-871f-4f83-87c7-8c6f9a2f4658/etc-hosts",
        "imageSubPath": "",
        "propagation": "PROPAGATION_PRIVATE",
        "readonly": false,
        "recursiveReadOnly": false,
        "selinuxRelabel": false,
        "uidMappings": []
      },
      {
        "containerPath": "/dev/termination-log",
        "gidMappings": [],
        "hostPath": "/var/lib/kubelet/pods/c28a4ef5-871f-4f83-87c7-8c6f9a2f4658/containers/pod-a/b8e4cdfb",
        "imageSubPath": "",
        "propagation": "PROPAGATION_PRIVATE",
        "readonly": false,
        "recursiveReadOnly": false,
        "selinuxRelabel": false,
        "uidMappings": []
      }
    ],
    "reason": "",
    "resources": {
      "linux": {
        "cpuPeriod": "100000",
        "cpuQuota": "0",
        "cpuShares": "2",
        "cpusetCpus": "",
        "cpusetMems": "",
        "hugepageLimits": [],
        "memoryLimitInBytes": "0",
        "memorySwapLimitInBytes": "0",
        "oomScoreAdj": "1000",
        "unified": {
          "memory.oom.group": "1",
          "memory.swap.max": "0"
        }
      }
    },
    "startedAt": "2026-01-13T21:25:52.459488036Z",
    "state": "CONTAINER_RUNNING",
    "stopSignal": "RUNTIME_DEFAULT",
    "user": {
      "linux": {
        "gid": "0",
        "supplementalGroups": [
          "0",
          "10"
        ],
        "uid": "0"
      }
    }
  }
}
root@dakube:~# crictl inspect 78004d5817093|grep proc
            "/proc/asound",
            "/proc/acpi",
            "/proc/interrupts",
            "/proc/kcore",
            "/proc/keys",
            "/proc/latency_stats",
            "/proc/timer_list",
            "/proc/timer_stats",
            "/proc/sched_debug",
            "/proc/scsi",
            "/proc/bus",
            "/proc/fs",
            "/proc/irq",
            "/proc/sys",
            "/proc/sysrq-trigger"
          "/proc/asound",
          "/proc/acpi",
          "/proc/interrupts",
          "/proc/kcore",
          "/proc/keys",
          "/proc/latency_stats",
          "/proc/timer_list",
          "/proc/timer_stats",
          "/proc/sched_debug",
          "/proc/scsi",
            "path": "/proc/82511/ns/ipc",
            "path": "/proc/82511/ns/uts",
            "path": "/proc/82511/ns/net",
          "/proc/bus",
          "/proc/fs",
          "/proc/irq",
          "/proc/sys",
          "/proc/sysrq-trigger"
          "destination": "/proc",
          "source": "proc",
          "type": "proc"
      "process": {
root@dakube:~# crictl inspect 78004d5817093|grep ns/net -b4
5920-          {
5932-            "type": "mount"
5960-          },
5973-          {
5985:            "path": "/proc/82511/ns/net",
6027-            "type": "network"
6057-          },
6070-          {
6082-            "type": "cgroup"
root@dakube:~# crictl inspect 78004d5817093|grep ns/net -c4
1
root@dakube:~# crictl inspect 78004d5817093|grep ns/net -a4
          {
            "type": "mount"
          },
          {
            "path": "/proc/82511/ns/net",
            "type": "network"
          },
          {
            "type": "cgroup"
root@dakube:~# crictl inspect 78004d5817093|grep ns/net -a10
            "type": "ipc"
          },
          {
            "path": "/proc/82511/ns/uts",
            "type": "uts"
          },
          {
            "type": "mount"
          },
          {
            "path": "/proc/82511/ns/net",
            "type": "network"
          },
          {
            "type": "cgroup"
          }
        ],
        "readonlyPaths": [
          "/proc/bus",
          "/proc/fs",
          "/proc/irq",
root@dakube:~# kubectl delete pod pod-a
pod "pod-a" deleted from default namespace